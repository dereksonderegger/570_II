<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Two-Sample Hypothesis Tests and Confidence Intervals | Introduction to Statistical Methodology, Second Edition</title>
  <meta name="description" content="Chapter 7 Two-Sample Hypothesis Tests and Confidence Intervals | Introduction to Statistical Methodology, Second Edition" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Two-Sample Hypothesis Tests and Confidence Intervals | Introduction to Statistical Methodology, Second Edition" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/570_II" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Two-Sample Hypothesis Tests and Confidence Intervals | Introduction to Statistical Methodology, Second Edition" />
  
  
  

<meta name="author" content="Derek L. Sonderegger &amp; Robert Buscaglia" />


<meta name="date" content="2019-10-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6-hypothesis-tests-for-the-mean-of-a-population.html"/>
<link rel="next" href="appendix-a-alternative-bootstrap-code.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html"><i class="fa fa-check"></i><b>1</b> Summary Statistics and Graphing</a><ul>
<li class="chapter" data-level="1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#variable-types"><i class="fa fa-check"></i><b>1.1</b> Variable Types</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#categorical"><i class="fa fa-check"></i><b>1.1.1</b> Categorical</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#numerical"><i class="fa fa-check"></i><b>1.1.2</b> Numerical</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#randomization-and-sampling"><i class="fa fa-check"></i><b>1.2</b> Randomization and Sampling</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#sampling-techniques"><i class="fa fa-check"></i><b>1.2.1</b> Sampling Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#graphical-summaries"><i class="fa fa-check"></i><b>1.3</b> Graphical Summaries</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#barchartsbarplots-univariate---categorical"><i class="fa fa-check"></i><b>1.3.1</b> Barcharts/Barplots (Univariate - Categorical)</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#histogram-univariate---numerical"><i class="fa fa-check"></i><b>1.3.2</b> Histogram (Univariate - Numerical)</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#boxplot-bivariate---categorical-vs-numerical"><i class="fa fa-check"></i><b>1.3.3</b> Boxplot (Bivariate - Categorical vs Numerical)</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#scatterplot-bivariate---numerical-vs-numerical"><i class="fa fa-check"></i><b>1.3.4</b> Scatterplot (Bivariate - Numerical vs Numerical)</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-centrality"><i class="fa fa-check"></i><b>1.4</b> Measures of Centrality</a><ul>
<li class="chapter" data-level="1.4.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mean"><i class="fa fa-check"></i><b>1.4.1</b> Mean</a></li>
<li class="chapter" data-level="1.4.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#median"><i class="fa fa-check"></i><b>1.4.2</b> Median</a></li>
<li class="chapter" data-level="1.4.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mode"><i class="fa fa-check"></i><b>1.4.3</b> Mode</a></li>
<li class="chapter" data-level="1.4.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#examples"><i class="fa fa-check"></i><b>1.4.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-spread"><i class="fa fa-check"></i><b>1.5</b> Measures of Spread</a><ul>
<li class="chapter" data-level="1.5.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#range"><i class="fa fa-check"></i><b>1.5.1</b> Range</a></li>
<li class="chapter" data-level="1.5.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#inter-quartile-range-iqr"><i class="fa fa-check"></i><b>1.5.2</b> Inter-Quartile Range (IQR)</a></li>
<li class="chapter" data-level="1.5.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#variance"><i class="fa fa-check"></i><b>1.5.3</b> Variance</a></li>
<li class="chapter" data-level="1.5.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#standard-deviation"><i class="fa fa-check"></i><b>1.5.4</b> Standard Deviation</a></li>
<li class="chapter" data-level="1.5.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#coefficient-of-variation"><i class="fa fa-check"></i><b>1.5.5</b> Coefficient of Variation</a></li>
<li class="chapter" data-level="1.5.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#empirical-rules"><i class="fa fa-check"></i><b>1.5.6</b> Empirical Rules</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#shape"><i class="fa fa-check"></i><b>1.6</b> Shape</a><ul>
<li class="chapter" data-level="1.6.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#symmetry"><i class="fa fa-check"></i><b>1.6.1</b> Symmetry</a></li>
<li class="chapter" data-level="1.6.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#unimodal-or-multi-modal"><i class="fa fa-check"></i><b>1.6.2</b> Unimodal or Multi-modal</a></li>
<li class="chapter" data-level="1.6.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#skew"><i class="fa fa-check"></i><b>1.6.3</b> Skew</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>1.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-probability.html"><a href="2-probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-set-theory"><i class="fa fa-check"></i><b>2.1</b> Introduction to Set Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-probability.html"><a href="2-probability.html#composition-of-events"><i class="fa fa-check"></i><b>2.1.1</b> Composition of events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-probability.html"><a href="2-probability.html#probability-rules"><i class="fa fa-check"></i><b>2.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-probability.html"><a href="2-probability.html#simple-rules"><i class="fa fa-check"></i><b>2.2.1</b> Simple Rules</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-probability.html"><a href="2-probability.html#conditional-probability"><i class="fa fa-check"></i><b>2.2.2</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-probability.html"><a href="2-probability.html#summary-of-probability-rules"><i class="fa fa-check"></i><b>2.2.3</b> Summary of Probability Rules</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-probability.html"><a href="2-probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>2.3</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-discrete-random-variables"><i class="fa fa-check"></i><b>2.3.1</b> Introduction to Discrete Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-probability.html"><a href="2-probability.html#common-discrete-distributions"><i class="fa fa-check"></i><b>2.4</b> Common Discrete Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-probability.html"><a href="2-probability.html#binomial-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Binomial Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-probability.html"><a href="2-probability.html#poisson-distribution"><i class="fa fa-check"></i><b>2.4.2</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-probability.html"><a href="2-probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-probability.html"><a href="2-probability.html#uniform01-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-probability.html"><a href="2-probability.html#exponential-distribution"><i class="fa fa-check"></i><b>2.5.2</b> Exponential Distribution</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-probability.html"><a href="2-probability.html#normal-distribution"><i class="fa fa-check"></i><b>2.5.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-probability.html"><a href="2-probability.html#standardizing"><i class="fa fa-check"></i><b>2.5.4</b> Standardizing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-probability.html"><a href="2-probability.html#r-quick-reference"><i class="fa fa-check"></i><b>2.6</b> R Quick Reference</a></li>
<li class="chapter" data-level="2.7" data-path="2-probability.html"><a href="2-probability.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html"><i class="fa fa-check"></i><b>3</b> Confidence Intervals via Bootstrapping</a><ul>
<li class="chapter" data-level="3.1" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#theory-of-bootstrapping"><i class="fa fa-check"></i><b>3.1</b> Theory of Bootstrapping</a></li>
<li class="chapter" data-level="3.2" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#conducting-a-bootstrap"><i class="fa fa-check"></i><b>3.2</b> Conducting a Bootstrap</a></li>
<li class="chapter" data-level="3.3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#quantile-based-confidence-intervals"><i class="fa fa-check"></i><b>3.3</b> Quantile-based Confidence Intervals</a></li>
<li class="chapter" data-level="3.4" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#additional-examples"><i class="fa fa-check"></i><b>3.4</b> Additional Examples</a></li>
<li class="chapter" data-level="3.5" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html"><i class="fa fa-check"></i><b>4</b> Sampling Distribution of <span class="math inline">\(\bar{X}\)</span></a><ul>
<li class="chapter" data-level="4.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#enlightening-example"><i class="fa fa-check"></i><b>4.1</b> Enlightening Example</a></li>
<li class="chapter" data-level="4.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mathematical-details"><i class="fa fa-check"></i><b>4.2</b> Mathematical details</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#probability-rules-for-expectations-and-variances"><i class="fa fa-check"></i><b>4.2.1</b> Probability Rules for Expectations and Variances</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i><b>4.2.2</b> Mean and Variance of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#distribution-of-barx"><i class="fa fa-check"></i><b>4.3</b> Distribution of <span class="math inline">\(\bar{X}\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#central-limit-theorem"><i class="fa fa-check"></i><b>4.4</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="4.5" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#exercises-3"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals for <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotic-result-sigma-known"><i class="fa fa-check"></i><b>5.1</b> Asymptotic result (<span class="math inline">\(\sigma\)</span> known)</a></li>
<li class="chapter" data-level="5.2" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotoic-result-sigma-unknown"><i class="fa fa-check"></i><b>5.2</b> Asymptotoic result (<span class="math inline">\(\sigma\)</span> unknown)</a></li>
<li class="chapter" data-level="5.3" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#sample-size-selection"><i class="fa fa-check"></i><b>5.3</b> Sample Size Selection</a></li>
<li class="chapter" data-level="5.4" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#exercises-4"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests for the mean of a population</a><ul>
<li class="chapter" data-level="6.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#writing-hypotheses"><i class="fa fa-check"></i><b>6.1</b> Writing Hypotheses</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>6.1.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#error"><i class="fa fa-check"></i><b>6.1.2</b> Error</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#conducting-a-hypothesis-test-for-mu"><i class="fa fa-check"></i><b>6.2</b> Conducting a Hypothesis Test for <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#why-should-hypotheses-use-mu-and-not-barx"><i class="fa fa-check"></i><b>6.2.1</b> Why should hypotheses use <span class="math inline">\(\mu\)</span> and not <span class="math inline">\(\bar{x}\)</span>?</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#a-note-on-calculating-p-values"><i class="fa fa-check"></i><b>6.2.2</b> A note on calculating p-values</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#additional-examples-1"><i class="fa fa-check"></i><b>6.3</b> Additional Examples</a></li>
<li class="chapter" data-level="6.4" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#p-values-vs-cutoff-values"><i class="fa fa-check"></i><b>6.4</b> P-values vs cutoff values</a></li>
<li class="chapter" data-level="6.5" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#running-a-t-test-in-r"><i class="fa fa-check"></i><b>6.5</b> Running a t-test in R</a></li>
<li class="chapter" data-level="6.6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>6.6</b> Type I and Type II Errors</a><ul>
<li class="chapter" data-level="6.6.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#power-and-sample-size-selection"><i class="fa fa-check"></i><b>6.6.1</b> Power and Sample Size Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><i class="fa fa-check"></i><b>7</b> Two-Sample Hypothesis Tests and Confidence Intervals</a><ul>
<li class="chapter" data-level="7.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups"><i class="fa fa-check"></i><b>7.1</b> Difference in means between two groups</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-resampling"><i class="fa fa-check"></i><b>7.1.1</b> Inference via resampling</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-unequal-variance-assumption"><i class="fa fa-check"></i><b>7.1.2</b> Inference via asymptotic results (unequal variance assumption)</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-equal-variance-assumption"><i class="fa fa-check"></i><b>7.1.3</b> Inference via asymptotic results (equal variance assumption)</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#additional-example"><i class="fa fa-check"></i><b>7.1.4</b> Additional Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups-paired-data"><i class="fa fa-check"></i><b>7.2</b> Difference in means between two groups: Paired Data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#additional-example-1"><i class="fa fa-check"></i><b>7.2.1</b> Additional Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#exercises-6"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-alternative-bootstrap-code.html"><a href="appendix-a-alternative-bootstrap-code.html"><i class="fa fa-check"></i>Appendix A : Alternative Bootstrap Code</a><ul>
<li class="chapter" data-level="" data-path="appendix-a-alternative-bootstrap-code.html"><a href="appendix-a-alternative-bootstrap-code.html#mosaic-package"><i class="fa fa-check"></i>Mosaic Package</a></li>
<li class="chapter" data-level="" data-path="appendix-a-alternative-bootstrap-code.html"><a href="appendix-a-alternative-bootstrap-code.html#base-r-code"><i class="fa fa-check"></i>Base R Code</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Methodology, Second Edition</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="two-sample-hypothesis-tests-and-confidence-intervals" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Two-Sample Hypothesis Tests and Confidence Intervals</h1>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" title="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb140-2" title="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb140-3" title="3"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb140-4" title="4"><span class="kw">library</span>(boot)</a>
<a class="sourceLine" id="cb140-5" title="5"></a>
<a class="sourceLine" id="cb140-6" title="6"><span class="co"># Set default behavior of ggplot2 graphs to be black/white theme</span></a>
<a class="sourceLine" id="cb140-7" title="7"><span class="kw">theme_set</span>(<span class="kw">theme_bw</span>())</a></code></pre></div>
<p>There are two broad classification types for research, observational studies and designed experiments. These two types of research differ in the way that the researcher interacts with the subjects being observed. In an observational study, the researcher doesn’t force a subject into some behavior or treatment, but merely observes the subject (making measurements but not changing behaviors). In contrast, in an experiment, the researcher imposes different treatments onto the subjects and the pairing between the subject and treatment group happens at random.</p>
<p><strong>Example:</strong> For many years hormone (Estrogen and Progestin) replacement therapy’s primary use for post-menopausal woman was to reduce the uncomfortable side-effects of menopause but it was thought to also reduced the rate of breast cancer in post-menopausal women. This belief was the result of many observational studies where women who chose to take hormone replacement therapy also had reduced rates of breast cancer. The lurking variable that the observational studies missed was that hormone therapy is relatively expensive and was taken by predominately women of a high socio- economic status. Those women tended to be more health conscious, lived in areas with less pollution, and were generally at a lower risk for developing breast cancer. Even when researchers realized that socio-economic status was confounded with the therapy, they couldn’t be sure which was the cause of the reduced breast cancer rates. Two variables are said to be confounded if the design of a given experiment or study cannot distinguish the effect of one variable from the other. To correctly test this, nearly 17,000 women underwent an experiment in which each women was randomly assigned to take either the treatment (E+P) or a placebo. The Women’s Health Initiative (WHI) Estrogen plus Progestin Study (E+P) was stopped on July 7, 2002 (after an average 5.6 years of follow-up) because of increased risks of cardiovascular disease and breast cancer in women taking active study pills, compared with those on placebo (inactive pills). The study showed that the overall risks exceeded the benefits, with women taking E+P at higher risk for heart disease, blood clots, stroke, and breast cancer, but at lower risk for fracture and colon cancer. Lurking variables such as income levels and education are correlated to overall health behaviors and with an increased use of hormone replacement therapy. By randomly assigning each woman to a treatment, the unidentified lurking variables were evenly spread across treatments and the dangers of hormone replacement therapy were revealed.</p>
<p>In the previous paragraph, we introduced the idea of a <strong>lurking variable</strong> where a lurking variable is a variable the researcher hasn’t considered but affects the response variable. In observational studies a researcher will try to measure all the variables that might affect the response but will undoubtedly miss something.</p>
<p>There is a fundamental difference between imposing treatments onto subjects versus taking a random sample from a population and observing relationships between variables. In general, designed experiments allow us to determine cause-and-effect relationships while observational studies can only determine if variables are correlated. This difference in how the data is generated will result in different methods for generating a sampling distribution for a statistic of interest. In this chapter we will focus on experimental designs, though the same analyses are appropriate for observational studies.</p>
<div id="difference-in-means-between-two-groups" class="section level2">
<h2><span class="header-section-number">7.1</span> Difference in means between two groups</h2>
<p>Often researchers will obtain a group of subjects and divide them into two groups, provide different treatments to each, and observe some response. The goal is to see if the two groups have different mean values, as this is the most common difference to be interested in.</p>
<p>The first thing to consider is that the group of subjects in our sample should be representative of a population of interest. Because we cannot impose an experiment on an entire population, we often are forced to examine a small sample and we hope that the sample statistics (the sample mean <span class="math inline">\(\bar{x}\)</span>, and sample standard deviation <span class="math inline">\(s\)</span>) are good estimates of the population parameters (the population mean <span class="math inline">\(\mu\)</span>, and population standard deviation <span class="math inline">\(\sigma\)</span>). First recognize that these are a sample and we generally think of them to be representative of some population.</p>
<p><strong>Example:</strong> Finger Tapping and Caffeine</p>
<p>The effects of caffeine on the body have been well studied. In one experiment, a group of male college students were trained in a particular tapping movement and to tap at a rapid rate. They were randomly divided into caffeine and non-caffeine groups and given approximately two cups of coffee (with either 200 mg of caffeine or none). After a 2-hour period, the students tapping rate was measured.</p>
<p>The population that we are trying to learn about is male college-aged students and we the most likely question of interest is if the mean tap rate of the caffeinated group is different than the non-caffeinated group. Notice that we want to take this sample of 20 students to make inference on the population of male college-aged students. The hypotheses we are interested in are
<span class="math display">\[\begin{aligned} 
H_{0}:  \mu_{nc} &amp;=   \mu_{c} \\    
H_{a}:  \mu_{nc} &amp;\ne \mu_{c}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\mu_{c}\)</span> is the mean tap rate of the caffeinated group and <span class="math inline">\(\mu_{nc}\)</span> is the mean tap rate of the non-caffeinated group. We could equivalently express these hypotheses via
<span class="math display">\[\begin{aligned}
H_{0}:  \mu_{nc}-\mu_{c}  &amp;=  0  \\
H_{a}:  \mu_{nc}-\mu_{c} &amp;\ne 0
\end{aligned}\]</span></p>
<p>Or we could let <span class="math inline">\(\delta=\mu_{nc}-\mu_{c}\)</span> and write the hypotheses as
<span class="math display">\[\begin{aligned}
H_{0}:\,\delta  &amp;=    0 \\
H_{a}:\,\delta  &amp;\ne    0
\end{aligned}\]</span></p>
<p>The data are available in many different formats at <a href="http://www.lock5stat.com/datapage.html" class="uri">http://www.lock5stat.com/datapage.html</a></p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" title="1"><span class="kw">data</span>(CaffeineTaps, <span class="dt">package=</span><span class="st">&#39;Lock5Data&#39;</span>)   <span class="co"># load the data from the Lock5Data package</span></a>
<a class="sourceLine" id="cb141-2" title="2"><span class="kw">str</span>(CaffeineTaps)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    20 obs. of  2 variables:
##  $ Taps : int  246 248 250 252 248 250 246 248 245 250 ...
##  $ Group: Factor w/ 2 levels &quot;Caffeine&quot;,&quot;NoCaffeine&quot;: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>The dataset contains two variables. Taps are the response of interest. Group is a factor (or categorical variable) that has 2 levels. These are the different groupings of Caffeine and NoCaffeine. The first thing we should do is, as always, graph the data.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" title="1"><span class="kw">ggplot</span>(CaffeineTaps, <span class="kw">aes</span>(<span class="dt">x=</span>Taps)) <span class="op">+</span></a>
<a class="sourceLine" id="cb143-2" title="2"><span class="st">  </span><span class="kw">geom_dotplot</span>(<span class="dt">binwidth=</span>.<span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb143-3" title="3"><span class="st">  </span><span class="kw">facet_grid</span>(Group <span class="op">~</span><span class="st"> </span>.)  <span class="co"># two graphs stacked by Group (Caffeine vs non)</span></a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>From this view, it looks like the caffeine group has a higher tapping rate. It will be helpful to summarize the difference between these two groups with a single statistic by calculating the mean for each group and then calculate the difference between the group means.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" title="1">CaffeineTaps <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb144-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(Group) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># group the summary stats by Treatment group</span></a>
<a class="sourceLine" id="cb144-3" title="3"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">xbar=</span><span class="kw">mean</span>(Taps), <span class="dt">s=</span><span class="kw">sd</span>(Taps))</a></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   Group       xbar     s
##   &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1 Caffeine    248.  2.21
## 2 NoCaffeine  245.  2.39</code></pre>
<p>We can find then find the difference in the sample means.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" title="1"><span class="co"># No Caffeine  -  Caffeine</span></a>
<a class="sourceLine" id="cb146-2" title="2"><span class="co"># 244.8 - 248.3</span></a>
<a class="sourceLine" id="cb146-3" title="3">CaffeineTaps <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(Group) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb146-4" title="4"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">xbar=</span><span class="kw">mean</span>(Taps)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb146-5" title="5"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">d =</span> <span class="kw">diff</span>(xbar))</a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##       d
##   &lt;dbl&gt;
## 1  -3.5</code></pre>
<p>Notationally, lets call this statistic <span class="math inline">\(d=\bar{x}_{nc}-\bar{x}_{c}=-3.5\)</span>. We are interested in testing if this observed difference might be due to just random chance and we just happened to assigned more of the fast tappers to the caffeine group. How could we test the null hypothesis that the mean of the caffeinated group is different than the non-caffeinated?</p>
<div id="inference-via-resampling" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Inference via resampling</h3>
<p>The key idea is “How could the data have turned out if the null hypothesis is true?” If the null hypothesis is true, then the caffeinated/non-caffeinated group treatment had no effect on the tap rate and it was just random chance that the caffeinated group got a larger percentage of fast tappers. That is to say the group variable has no relationship to tap rate. I could have just as easily assigned the fast tappers to the non-caffeinated group purely by random chance. So our simulation technique is <em>to shuffle the group labels and then calculate a difference between the group means</em>!</p>
<p>Below we demonstrate what it would look like to shuffle the groups. This is the core concept behind the permutation methods, and how we can work to make an inference via resampling.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" title="1"><span class="co"># shuffle(): takes an input column and reorders it randomly</span></a>
<a class="sourceLine" id="cb148-2" title="2">CaffeineTaps <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">ShuffledGroup =</span> mosaic<span class="op">::</span><span class="kw">shuffle</span>(Group))</a></code></pre></div>
<pre><code>##    Taps      Group ShuffledGroup
## 1   246   Caffeine      Caffeine
## 2   248   Caffeine    NoCaffeine
## 3   250   Caffeine    NoCaffeine
## 4   252   Caffeine      Caffeine
## 5   248   Caffeine    NoCaffeine
## 6   250   Caffeine      Caffeine
## 7   246   Caffeine    NoCaffeine
## 8   248   Caffeine      Caffeine
## 9   245   Caffeine    NoCaffeine
## 10  250   Caffeine      Caffeine
## 11  242 NoCaffeine      Caffeine
## 12  245 NoCaffeine      Caffeine
## 13  244 NoCaffeine    NoCaffeine
## 14  248 NoCaffeine      Caffeine
## 15  247 NoCaffeine    NoCaffeine
## 16  248 NoCaffeine    NoCaffeine
## 17  242 NoCaffeine      Caffeine
## 18  244 NoCaffeine    NoCaffeine
## 19  246 NoCaffeine    NoCaffeine
## 20  242 NoCaffeine      Caffeine</code></pre>
<p>We can then calculate the mean difference but this time using the randomly generated groups, and now the non-caffeinated group just happens to have a slightly higher mean tap rate just by the random sorting into two groups.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" title="1">CaffeineTaps <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb150-2" title="2"><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">ShuffledGroup =</span> mosaic<span class="op">::</span><span class="kw">shuffle</span>(Group) ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb150-3" title="3"><span class="st">  </span><span class="kw">group_by</span>( ShuffledGroup )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb150-4" title="4"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">xbar=</span><span class="kw">mean</span>(Taps)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb150-5" title="5"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">d.star =</span> <span class="kw">diff</span>(xbar)) </a></code></pre></div>
<pre><code>## # A tibble: 1 x 1
##   d.star
##    &lt;dbl&gt;
## 1   -0.7</code></pre>
<p>We could repeat this shuffling several times and see the possible values we might have seen if the null hypothesis is correct and the treatment group doesn’t matter at all.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" title="1">mosaic<span class="op">::</span><span class="kw">do</span>(<span class="dv">5</span>) <span class="op">*</span><span class="st"> </span>{</a>
<a class="sourceLine" id="cb152-2" title="2">  CaffeineTaps <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb152-3" title="3"><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">ShuffledGroup =</span> mosaic<span class="op">::</span><span class="kw">shuffle</span>(Group) ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb152-4" title="4"><span class="st">  </span><span class="kw">group_by</span>( ShuffledGroup )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb152-5" title="5"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">xbar=</span><span class="kw">mean</span>(Taps)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb152-6" title="6"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">d.star =</span> <span class="kw">diff</span>(xbar))  </a>
<a class="sourceLine" id="cb152-7" title="7">}</a></code></pre></div>
<pre><code>##   d.star
## 1   -1.5
## 2   -1.7
## 3   -0.3
## 4    2.3
## 5   -0.3</code></pre>
<p>Of course, five times isn’t sufficient to understand the sampling distribution of the mean difference under the null hypothesis, we should do more.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb154-1" title="1">PermutationDist &lt;-<span class="st"> </span>mosaic<span class="op">::</span><span class="kw">do</span>(<span class="dv">10000</span>) <span class="op">*</span><span class="st"> </span>{</a>
<a class="sourceLine" id="cb154-2" title="2">  CaffeineTaps <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb154-3" title="3"><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">ShuffledGroup =</span> mosaic<span class="op">::</span><span class="kw">shuffle</span>(Group) ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb154-4" title="4"><span class="st">  </span><span class="kw">group_by</span>( ShuffledGroup )  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb154-5" title="5"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">xbar=</span><span class="kw">mean</span>(Taps)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb154-6" title="6"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">d.star =</span> <span class="kw">diff</span>(xbar))  </a>
<a class="sourceLine" id="cb154-7" title="7">}</a></code></pre></div>
<p>We can then take the results of our 10000 permutations and view a histogram of the resulting difference in the shuffled group means (<span class="math inline">\(d^*\)</span>).</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" title="1"><span class="kw">ggplot</span>(PermutationDist, <span class="kw">aes</span>(<span class="dt">x=</span>d.star)) <span class="op">+</span></a>
<a class="sourceLine" id="cb155-2" title="2"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span>.<span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb155-3" title="3"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Permutation dist. of d* assuming H0 is true&#39;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb155-4" title="4"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&#39;d*&#39;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb155-5" title="5"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">c</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), <span class="dt">lwd=</span><span class="fl">1.5</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)</a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We are then interested in how often from our permutations did we observe something more extreme than the mean difference from the original groupings. Because this is a two-tailed test, we will look for how many observations are either below -3.5 or above +3.5. The original difference in the means are marked as vertical red lines in the graph above.</p>
<p>We have almost no cases where the randomly assigned groups produced a difference as extreme as the actual observed difference of <span class="math inline">\(d=-3.5\)</span>. We can calculate the percentage of the sampling distribution of the difference in means that is farther from zero</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb156-1" title="1">PermutationDist <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb156-2" title="2"><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">MoreExtreme =</span> <span class="kw">ifelse</span>( <span class="kw">abs</span>(d.star) <span class="op">&gt;=</span><span class="st"> </span><span class="fl">3.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb156-3" title="3"><span class="st">  </span><span class="kw">summarise</span>( <span class="dt">p.value =</span> <span class="kw">mean</span>(MoreExtreme))</a></code></pre></div>
<pre><code>##   p.value
## 1  0.0058</code></pre>
<p>We see that only 58/10,000 simulations of data produced assuming <span class="math inline">\(H_{0}\)</span> is true produced a <span class="math inline">\(d^{*}\)</span> value more extreme than our observed difference in sample means. This is exactly the definition we have given to a p-value; thus, we can reject the null hypothesis <span class="math inline">\(H_{0}:\mu_{nc}-\mu_{c}=0\)</span> in favor of the alternative <span class="math inline">\(H_{a}:\mu_{nc}-\mu_{c}\ne 0\)</span> at an <span class="math inline">\(\alpha=0.05\)</span> or any other reasonable <span class="math inline">\(\alpha\)</span> level.</p>
<div id="using-coin" class="section level4">
<h4><span class="header-section-number">7.1.1.1</span> Using coin</h4>
<p>To make the code less cumbersome, we can incorporate the use of the <code>coin</code> package. This package will allow us to perform a variety of permutation tests without having to produce code such as that shown above. We will only need to ensure that our data is prepared properly. However, for those who are interested more in the R coding that can be done to produce permutation tests, please see Appendix B : Alternative Permutation Test Code.</p>
<p>The data in <code>CaffeineTaps</code> has the data separated as <code>Taps</code> and <code>Group</code>, which is exactly the form we need it in. We can run the permutation using <code>coin</code> simply by using the <code>oneway_test()</code> command and asking it to approximate the p-value. It will then run the permutation test for us. The same number of reshuffles as above (10000) is used.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb158-1" title="1"><span class="kw">library</span>(coin)</a>
<a class="sourceLine" id="cb158-2" title="2"><span class="kw">oneway_test</span>(Taps<span class="op">~</span>Group, <span class="dt">data=</span>CaffeineTaps, <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>,</a>
<a class="sourceLine" id="cb158-3" title="3">            <span class="dt">distribution=</span><span class="kw">approximate</span>(<span class="dt">nresample=</span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>))</a></code></pre></div>
<pre><code>## 
##  Approximative Two-Sample Fisher-Pitman Permutation Test
## 
## data:  Taps by Group (Caffeine, NoCaffeine)
## Z = 2.723, p-value = 0.0053
## alternative hypothesis: true mu is not equal to 0</code></pre>
<p>We observe excellent agreement to the simulation run above, but with much less involvement on how to handle the code.</p>
</div>
<div id="different-alternative-hypothesis" class="section level4">
<h4><span class="header-section-number">7.1.1.2</span> Different Alternative Hypothesis</h4>
<p>Everything we know about the biological effects of ingesting caffeine suggests that we should have expected the caffeinated group to tap faster. We might want to set up our experiment so only faster tapping represents “extreme” data compared to the null hypothesis. In this case we want an alternative of <span class="math inline">\(H_{a}:\,\mu_{nc}-\mu_{c}&lt;0\)</span> We can state our null and alternative hypothesis as
<span class="math display">\[\begin{aligned}
H_{0}:\,\mu_{nc}-\mu_{c}    &amp;\ge    0 \\
H_{a}:\,\mu_{nc}-\mu_{c}    &amp;&lt;  0
\end{aligned}\]</span></p>
<p>The creation of the sampling distribution of the mean difference <span class="math inline">\(d^*\)</span> is identical to our previous technique because if our observed difference <span class="math inline">\(d\)</span> is so negative that it is incompatible with the hypothesis that <span class="math inline">\(\mu_{nc}-\mu_{c}=0\)</span> then it must also be incompatible with any positive difference. We can perform the permutation test and generate the distribution of estimated differences in the same manner as above. The only difference in the analysis is at the end when we calculate the p-value and don’t consider the positive tail. That is, the p-value is the percent of simulations where <span class="math inline">\(d^*&lt;d\)</span>.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb160-1" title="1">PermutationDist <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb160-2" title="2"><span class="st">  </span><span class="kw">summarize</span>( <span class="dt">p.value =</span> <span class="kw">mean</span>( d.star <span class="op">&lt;=</span><span class="st"> </span><span class="fl">-3.5</span> ))</a></code></pre></div>
<pre><code>##   p.value
## 1  0.0028</code></pre>
<p>We can perform a left-tailed test using <code>coin</code>, but need to be sure we call ‘NoCaffeine’ the first group. We can do this with <code>relevel()</code>.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" title="1">CaffeineTaps<span class="op">$</span>Group &lt;-<span class="st"> </span><span class="kw">relevel</span>(CaffeineTaps<span class="op">$</span>Group, <span class="st">&#39;NoCaffeine&#39;</span>)</a>
<a class="sourceLine" id="cb162-2" title="2"><span class="kw">oneway_test</span>(Taps<span class="op">~</span>Group, <span class="dt">data=</span>CaffeineTaps, <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>,</a>
<a class="sourceLine" id="cb162-3" title="3">            <span class="dt">distribution=</span><span class="kw">approximate</span>(<span class="dt">nresample=</span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>))</a></code></pre></div>
<pre><code>## 
##  Approximative Two-Sample Fisher-Pitman Permutation Test
## 
## data:  Taps by Group (NoCaffeine, Caffeine)
## Z = -2.723, p-value = 0.0022
## alternative hypothesis: true mu is less than 0</code></pre>
<p>From both methods we see that the p-value is approximately cut in half by ignoring the upper tail, which makes sense considering the observed symmetry in the sampling distribution of <span class="math inline">\(d^*\)</span>.</p>
<p>In general, we prefer to use a two-sided test because if the two-sided test leads us to reject the null hypothesis then so would the appropriate one-sided hypothesis (except in the case where the alternative was chosen before the data was collected and the observed data was in the other tail). Second, by using a two-sample test, it prevents us from from “tricking” ourselves when we don’t know the which group should have a higher mean going into the experiment, but after seeing the data, thinking we should have known and using the less stringent test. Some statisticians go so far as to say that using a 1-sided test is outright fraudulent. Generally, we’ll concentrate on two-sided tests as they are the most widely acceptable.</p>
</div>
<div id="inference-via-bootstrap-confidence-interval" class="section level4">
<h4><span class="header-section-number">7.1.1.3</span> Inference via Bootstrap Confidence Interval</h4>
<p>Just as we could use bootstrapping to evaluate a confidence interval for one-sample, we can do the same for two-samples. We need only update the function we are give the <code>boot</code> function.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" title="1">diff.mean.function &lt;-<span class="st"> </span><span class="cf">function</span>(data, index){</a>
<a class="sourceLine" id="cb164-2" title="2">  m1 =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">subset</span>(data[index, <span class="dv">1</span>], data[index, <span class="dv">2</span>] <span class="op">==</span><span class="st"> </span><span class="kw">levels</span>(data[,<span class="dv">2</span>])[<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb164-3" title="3">  m2 =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">subset</span>(data[index, <span class="dv">1</span>], data[index, <span class="dv">2</span>] <span class="op">==</span><span class="st"> </span><span class="kw">levels</span>(data[,<span class="dv">2</span>])[<span class="dv">2</span>]))</a>
<a class="sourceLine" id="cb164-4" title="4">  <span class="kw">return</span>(m1 <span class="op">-</span><span class="st"> </span>m2)</a>
<a class="sourceLine" id="cb164-5" title="5">}</a></code></pre></div>
<p>This function works slightly different than the Chapter 3 version. We must now ensure that we give it a data.frame where the first column are the observations and the second column the factored group labels. This code will then calculate the difference in the means while bootstrapping the elements observed. We can run the bootstrap in a nearly identical fashion to Chapter 3. Notice my <code>data</code> is no longer a vector of values, but the data.frame we have been working with.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" title="1">BootDist &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> CaffeineTaps, <span class="dt">statistic =</span> diff.mean.function, <span class="dt">R=</span><span class="dv">10000</span>)</a></code></pre></div>
<p>We can visualize the results identical to the earlier chapters, but now recognizing this sampling distribution represents the difference in the means of the NoCaffeine and Caffeine groups.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb166-1" title="1">BootDist.graph &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">dbar=</span>BootDist<span class="op">$</span>t)</a>
<a class="sourceLine" id="cb166-2" title="2"><span class="kw">ggplot</span>(BootDist.graph, <span class="kw">aes</span>(<span class="dt">x=</span>dbar)) <span class="op">+</span></a>
<a class="sourceLine" id="cb166-3" title="3"><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb166-4" title="4"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Difference in Mean NoCaffeine Taps and Mean Caffeine Taps&#39;</span>)</a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" title="1">CI &lt;-<span class="st"> </span><span class="kw">quantile</span>( BootDist<span class="op">$</span>t, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>) )</a>
<a class="sourceLine" id="cb167-2" title="2">CI</a></code></pre></div>
<pre><code>##      2.5%     97.5% 
## -5.454545 -1.505051</code></pre>
<p>Thus, we can state that with 95% confidence the difference between the mean NoCaffeine taps and mean Caffeine taps is between -5.4 and -1.5 taps. Notice that the null hypothesis value, <span class="math inline">\(\delta=0\)</span>, is not a value supported by the data because 0 is not in the 95% confidence interval. A subtle point in the above bootstrap code does not re-sampled each group separately. Because the experimental protocol was to have 10 in each group, we might want to use bootstrap code that accounts for the correct design. For now, we might end up with 12 caffeinated and 8 decaffeinated subjects, which is data that our experimental design couldn’t have generated. This should have minimal consequence and our bootstraps can still be conducted relatively easy.</p>
</div>
</div>
<div id="inference-via-asymptotic-results-unequal-variance-assumption" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Inference via asymptotic results (unequal variance assumption)</h3>
<p>Previously we’ve seen that the Central Limit Theorem gives us a way to estimate the distribution of the sample mean. So it should be reasonable to assume that for our two groups (1=NoCaffeine, 2=Caffeine),
<span class="math display">\[\bar{X}_{1}\stackrel{\cdot}{\sim}N\left(\mu_{1},\, \frac{\sigma_{1}^{2}}{n_1}\right)\;\;\;\textrm{and}\;\;\;\bar{X}_{2}\stackrel{\cdot}{\sim}N\left(\mu_{2},\; \frac{\sigma_{2}^{2}}{n_2}\right)\]</span></p>
<p>It turns out that because <span class="math inline">\(\bar{X}_{C}\)</span> and <span class="math inline">\(\bar{X}_{NC}\)</span> both have approximately normal distributions, then the difference between them also does. This shouldn’t be too surprising after looking at the permutation and bootstrap distributions of the <span class="math inline">\(d^*\)</span> values.</p>
<p>So our hypothesis tests and confidence interval routine will follow a similar pattern as our one-sample tests, but we now need to figure out the correct standardization formula for the difference in means. The only difficulty will be figuring out what the appropriate standard deviation term <span class="math inline">\(\hat{\sigma}_{D}\)</span> should be.</p>
<p>Recall that if two random variables, A and B, are independent then <span class="math display">\[Var\left(A-B\right)=Var(A)+Var(B)\]</span>
and therefore
<span class="math display">\[\begin{aligned} Var\left(D\right) 
  &amp;=    Var\left(\bar{X}_{1}-\bar{X}_{2}\right) \\
    &amp;=  Var\left(\bar{X}_{1}\right)+Var\left(\bar{X}_{2}\right) \\
    &amp;=  \frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}
    \end{aligned}\]</span>
and finally we have
<span class="math display">\[StdErr\left(D\right)=\sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}\]</span>
and therefore my standardized value for the difference will be
<span class="math display">\[\begin{aligned} t_{\Delta}    
  &amp;=    \frac{\textrm{estimate}\,\,\,-\,\,\,\textrm{null hypothesized value}}{StdErr\left(\,\,\textrm{estimate}\,\,\right)} \\
    \end{aligned}\]</span></p>
<p>The test statistic under unequal variance conditions is given by</p>
<p><span class="math display">\[\begin{aligned} t_{\Delta}    
    &amp;=  \frac{\left(\bar{x}_{1}-\bar{x}_{2}\right)}{\sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}} \\
    \end{aligned}\]</span></p>
<p>For the data evaluated here, we thus have</p>
<p><span class="math display">\[\begin{aligned} t_{\Delta}
    &amp;=  \frac{\left(-3.5\right)-0}{\sqrt{\frac{2.39^{2}}{10}+\frac{2.21^{2}}{10}}} \\
    &amp;=  -3.39 
    \end{aligned}\]</span></p>
<p>This is somewhat painful, but reasonable. The last question is what t-distribution should we compare this to? Previously we’ve used <span class="math inline">\(df=n-1\)</span> but now we have two samples. So our degrees of freedom ought to be somewhere between <span class="math inline">\(\min\left(n_{1},n_{2}\right)-2=8\)</span> and <span class="math inline">\(\left(n_{1}+n_{2}\right)-1=19\)</span>.</p>
<p>There is no correct answer, but the best approximation to what it should be is called Satterthwaite’s Approximation. We will give this degree of freedom a special character, <span class="math inline">\(\Delta\)</span>, to keep it clear when we are using it.
<span class="math display">\[\Delta=\frac{\left(V_{1}+V_{2}\right)^{2}}{\frac{V_{1}^{2}}{n_{1}-1}+\frac{V_{2}^{2}}{n_{2}-1}}\]</span>
where
<span class="math display">\[V_{1}=\frac{s_{1}^{2}}{n_{1}}\;\;\textrm{and }\;\;V_{2}=\frac{s_{2}^{2}}{n_{2}}\]</span></p>
<p>So for our example we have</p>
<p><span class="math display">\[V_{1}=\frac{2.39^{2}}{10}=0.5712\;\;\;\textrm{and}\;\;\;V_{2}=\frac{2.21^{2}}{10}=0.4884\]</span>
and
<span class="math display">\[\Delta=\frac{\left(0.5712+0.4884\right)^{2}}{\frac{\left(0.5712\right)^{2}}{9}+\frac{\left(0.4884\right)^{2}}{9}}=17.89\]</span></p>
<p>So now we can compute our p-value as</p>
<p><span class="math display">\[\textrm{p.value}=P\left(T_{17.89}&lt;-3.39\right)\]</span></p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" title="1">mosaic<span class="op">::</span><span class="kw">xpt</span>(<span class="op">-</span><span class="fl">3.39</span>, <span class="dt">df=</span><span class="fl">17.89</span>, <span class="dt">ncp=</span><span class="dv">0</span>)</a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre><code>## [1] 0.00164277</code></pre>
<div id="confidence-interval" class="section level4">
<h4><span class="header-section-number">7.1.2.1</span> Confidence Interval</h4>
<p>Similar to the theory discussed earlier, we can calculate the asymptotic confidence interval for the difference in the means. Recall that in general the confidence interval is given by</p>
<p><span class="math display">\[\begin{aligned}
\textrm{Est}\;\; &amp;\pm\; t_{\Delta}^{1-\alpha/2}\;\textrm{StdErr}\left(\;\textrm{Est}\;\right)   \\  
\end{aligned}\]</span></p>
<p>For the difference of two means under unequal variance conditions, this can be written</p>
<p><span class="math display">\[\begin{aligned}
\left(\bar{x}_{1}-\bar{x}_{2}\right)  &amp;\pm  t_{\Delta}^{1-\alpha/2}\sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}} \\
\end{aligned}\]</span></p>
<p>Working this through for the data set evaluated here, we find obtain</p>
<p><span class="math display">\[\begin{aligned}
-3.5  &amp;\pm  2.10\sqrt{\frac{2.39^{2}}{10}+\frac{2.21^{2}}{10}} \\
-3.5  &amp;\pm  2.16 
\end{aligned}\]</span></p>
<p>Where the critical t-score was found at <span class="math inline">\(\Delta\)</span> degrees of freedom</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" title="1"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="fl">17.89</span>)</a></code></pre></div>
<pre><code>## [1] 2.101848</code></pre>
<p>Giving a 95% confidence interval for the difference in mean taps for NoCaffeine and Caffeine groups as</p>
<p><span class="math display">\[\left(-5.66,\;-1.34\right)\]</span></p>
<p>It is probably fair to say that this is an ugly calculation to do by hand. Fortunately it isn’t too hard to make R do these calculations for you. The function <code>t.test()</code> will accept two arguments, a vector of values from the first group and a vector from the second group. We can also give it a formula, which is good to start understanding. Here we use <code>Response ~ Predictors</code>, which will be important for understanding linear models. We want to test if the response <code>Taps</code> differs between the two <code>Group</code> levels.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" title="1"><span class="kw">t.test</span>(Taps <span class="op">~</span><span class="st"> </span>Group, <span class="dt">data=</span>CaffeineTaps)</a></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  Taps by Group
## t = -3.3942, df = 17.89, p-value = 0.003255
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -5.667384 -1.332616
## sample estimates:
## mean in group NoCaffeine   mean in group Caffeine 
##                    244.8                    248.3</code></pre>
</div>
</div>
<div id="inference-via-asymptotic-results-equal-variance-assumption" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Inference via asymptotic results (equal variance assumption)</h3>
<p>In the <code>CaffeineTaps</code> example, the standard deviations of each group are quite similar. Instead of thinking of the data as
<span class="math display">\[\bar{X}_{1}\stackrel{\cdot}{\sim}N\left(\mu_{1},\,\frac{\sigma_{1}^{2}}{n_1}\right)\;\;\;\textrm{and}\;\;\;\bar{X}_{2}\stackrel{\cdot}{\sim}N\left(\mu_{2},\;\frac{\sigma_{2}^{2}}{n_2}\right)\]</span>
we could consider the model where we assume that the variance term is the same for each sample.
<span class="math display">\[\bar{X}_{1}\stackrel{\cdot}{\sim}N\left(\mu_{1},\,\frac{\sigma^{2}}{n_1}\right)\;\;\;\textrm{and}\;\;\;\bar{X}_{2}\stackrel{\cdot}{\sim}N\left(\mu_{2},\; \frac{\sigma^{2}}{n_2}\right)\]</span></p>
<p>First, we can estimate <span class="math inline">\(\mu_{1}\)</span> and <span class="math inline">\(\mu_{2}\)</span> with the appropriate sample means <span class="math inline">\(\bar{x}_{1}\)</span> and <span class="math inline">\(\bar{x}_{2}\)</span>. Next we need to calculate an estimate of <span class="math inline">\(\sigma\)</span> using all of the data. First recall the formula for the sample variance for one group was
<span class="math display">\[s^{2}=\frac{1}{n-1}\left[\sum_{j=1}^{n}\left(x_{j}-\bar{x}\right)^{2}\right]\]</span></p>
<p>In the case with two samples, we want a similar formula but it should take into account data from both sample groups. Define the notation <span class="math inline">\(x_{1j}\)</span> to be the <span class="math inline">\(j\)</span>th observation of group 1, and <span class="math inline">\(x_{2j}\)</span> to be the <span class="math inline">\(j\)</span>th observation of group 2 and in general <span class="math inline">\(x_{ij}\)</span> as the <span class="math inline">\(j\)</span>th observation from group <span class="math inline">\(i\)</span>. We want to subtract each observation from the its appropriate sample mean and then, because we had to estimate two means, we need to subtract two degrees of freedom from the denominator.
<span class="math display">\[\begin{aligned} s_{pooled}^{2}    
  &amp;=    \frac{1}{n_{1}+n_{2}-2}\left[\sum_{j=1}^{n_{1}}\left(x_{1j}-\bar{x}_{1}\right)^{2}+\sum_{j=1}^{n_{2}}\left(x_{2j}-\bar{x}_{2}\right)^{2}\right] \\
    &amp;=  \frac{1}{n_{1}+n_{2}-2}\left[\sum_{j=1}^{n_{1}}e_{1j}^{2}+\sum_{j=1}^{n_{2}}e_{2j}^{2}\right]\\
    &amp;=  \frac{1}{n_{1}+n_{2}-2}\left[\sum_{i=1}^{2}\sum_{j=1}^{n_{i}}e_{ij}^{2}\right]
    \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\bar{x}_{1}\)</span> and <span class="math inline">\(\bar{x}_{2}\)</span> are the sample means and <span class="math inline">\(e_{ij}=x_{ij}-\bar{x}_{i}\)</span> is the residual error of the <span class="math inline">\(i,j\)</span> observation. A computationally convenient formula for this same quantity is
<span class="math display">\[s_{pooled}^{2}=\frac{1}{n_{1}+n_{2}-2}\left[\left(n_{1}-1\right)s_{1}^{2}+\left(n_{2}-1\right)s_{2}^{2}\right]\]</span></p>
<p>Finally we notice that this pooled estimate of the variance term <span class="math inline">\(\sigma^{2}\)</span> has <span class="math inline">\(n_{1}+n_{2}-2\)</span> degrees of freedom. <em>One benefit of the pooled procedure is that we don’t have to mess with the Satterthwaite’s approximate degrees of freedom.</em></p>
<p>Recall our test statistic in the unequal variance case was
<span class="math display">\[t_{\Delta} 
  =\frac{\left(\bar{x}_{1}-\bar{x}_{2}\right)-0}{\sqrt{\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}}\]</span>
Now in the equal variance case, we will use the pooled estimate of the variance term <span class="math inline">\(s_{pooled}^{2}\)</span> instead of <span class="math inline">\(s_{1}^{2}\)</span> and <span class="math inline">\(s_{2}^{2}\)</span>, and we have known <span class="math inline">\(df = (n_1 + n_2 - 2)\)</span>.</p>
<p><span class="math display">\[\begin{aligned} t_{n_{1}+n_{2}-2} 
  &amp;=    \frac{\left(\bar{x}_{1}-\bar{x}_{2}\right)-0}{\sqrt{\frac{s_{pool}^{2}}{n_{1}}+\frac{s_{pool}^{2}}{n_{2}}}} \\
    &amp;=  \frac{\left(\bar{x}_{1}-\bar{x}_{2}\right)-0}{s_{pool}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}}
  \end{aligned}\]</span></p>
<p>where we note that
<span class="math display">\[StdErr\left(\bar{X}_{1}-\bar{X}_{2}\right)=s_{pooled}\sqrt{\left(1/n_{1}\right)+\left(1/n_{2}\right)}\]</span></p>
<div id="caffeine-example" class="section level4">
<h4><span class="header-section-number">7.1.3.1</span> Caffeine Example</h4>
<p>We can now rework the analysis of the Caffeine data under equal variance assumptions, allowing us to pool our estimate of the variance.</p>
<p>Recall our hypothesis for the <code>CaffeineTaps</code> data
<span class="math display">\[\begin{aligned}
H_{0}:  &amp;\mu_{nc}-\mu_{c}  =  0   \\
H_{a}:  &amp;\mu_{nc}-\mu_{c} \ne 0 
\end{aligned}\]</span></p>
<p>First we have to calculate the summary statistics for each group.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" title="1">CaffeineTaps <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb175-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(Group) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb175-3" title="3"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">xbar.i =</span> <span class="kw">mean</span>(Taps),   <span class="co"># sample mean for each group</span></a>
<a class="sourceLine" id="cb175-4" title="4">            <span class="dt">s2.i   =</span> <span class="kw">var</span>(Taps),    <span class="co"># sample variances for each group</span></a>
<a class="sourceLine" id="cb175-5" title="5">            <span class="dt">s.i    =</span> <span class="kw">sd</span>(Taps),     <span class="co"># sample standard deviations for each group</span></a>
<a class="sourceLine" id="cb175-6" title="6">            <span class="dt">n.i    =</span> <span class="kw">n</span>()      )    <span class="co"># sample sizes for each group</span></a></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   Group      xbar.i  s2.i   s.i   n.i
##   &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 NoCaffeine   245.  5.73  2.39    10
## 2 Caffeine     248.  4.90  2.21    10</code></pre>
<p>We can then use the descriptive statistics to determine the pooled variance estimate <span class="math inline">\(\sigma_{pooled}\)</span>).</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" title="1">CaffeineTaps <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(Group) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb177-3" title="3"><span class="st">  </span><span class="kw">summarize</span>(  <span class="dt">n.i =</span> <span class="kw">n</span>(),             </a>
<a class="sourceLine" id="cb177-4" title="4">             <span class="dt">s2.i =</span> <span class="kw">var</span>(Taps) ) <span class="op">%&gt;%</span><span class="st">     </span></a>
<a class="sourceLine" id="cb177-5" title="5"><span class="st">  </span><span class="kw">summarize</span>( <span class="dt">s2.p =</span> <span class="kw">sum</span>( (n.i<span class="dv">-1</span>)<span class="op">*</span>s2.i ) <span class="op">/</span><span class="st"> </span>( <span class="kw">sum</span>(n.i)<span class="op">-</span><span class="dv">2</span> ),</a>
<a class="sourceLine" id="cb177-6" title="6">             <span class="dt">s.p  =</span> <span class="kw">sqrt</span>(s2.p) ) </a></code></pre></div>
<pre><code>## # A tibble: 1 x 2
##    s2.p   s.p
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  5.32  2.31</code></pre>
<p>Next we can calculate
<span class="math display">\[t_{18}=\frac{\left(244.8-248.3\right)-0}{2.31\sqrt{\frac{1}{10}+\frac{1}{10}}}=-3.39\]</span></p>
<p>Finally we estimate our p-value</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" title="1">p.value &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">3.39</span>, <span class="dt">df=</span><span class="dv">18</span>)   <span class="co"># 2-sided test, so multiply by 2</span></a>
<a class="sourceLine" id="cb179-2" title="2">p.value</a></code></pre></div>
<pre><code>## [1] 0.003262969</code></pre>
<p>The change in the assumption of the variance makes little difference for the Caffeine data set, and we can still conclude that there is a difference in the mean taps between the Caffeine and NoCaffeine groups.</p>
</div>
<div id="confidence-interval-1" class="section level4">
<h4><span class="header-section-number">7.1.3.2</span> Confidence Interval</h4>
<p>The associated <span class="math inline">\(95\%\)</span> confidence interval when working under the equal variance assumption is
<span class="math display">\[\left(\bar{x}_{1}-\bar{x}_{2}\right)\pm t_{n_{1}+n_{2}-2}^{1-\alpha/2}\;\left(s_{pool}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{2}}}\right)\]</span></p>
<p>We now find the critical t-score at the known degree of freedom</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" title="1"><span class="kw">qt</span>( <span class="fl">.975</span>, <span class="dt">df=</span><span class="dv">18</span> ) </a></code></pre></div>
<pre><code>## [1] 2.100922</code></pre>
<p>Then calculate the confidence interval</p>
<p><span class="math display">\[\begin{aligned}
-3.5 &amp;\pm 2.10\left(\,2.31\sqrt{\frac{1}{10}+\frac{1}{10}}\right) \\
-3.5 &amp;\pm 2.17 
\end{aligned}\]</span>
<span class="math display">\[\left(-5.67,\;-1.33\right)\]</span></p>
<p>This p-value and <span class="math inline">\(95\%\)</span> confidence interval are quite similar to the values we got in the case where we assumed unequal variances.</p>
<p>As usual, these calculations are pretty annoying to do by hand and we wish to instead do them using R. Again the function <code>t.test()</code> will do the annoying calculations for us. We must only state that we want to do the test under equal variance or <code>var.equal=TRUE</code>.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" title="1"><span class="co"># Do the t-test</span></a>
<a class="sourceLine" id="cb183-2" title="2"><span class="kw">t.test</span>( Taps <span class="op">~</span><span class="st"> </span>Group, <span class="dt">data=</span>CaffeineTaps, <span class="dt">var.equal=</span><span class="ot">TRUE</span> ) </a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Taps by Group
## t = -3.3942, df = 18, p-value = 0.003233
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -5.66643 -1.33357
## sample estimates:
## mean in group NoCaffeine   mean in group Caffeine 
##                    244.8                    248.3</code></pre>
<p>Maybe we would like to evaluate a highly confidence level.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" title="1"><span class="co"># Do the t-test at 99% confidence</span></a>
<a class="sourceLine" id="cb185-2" title="2"><span class="kw">t.test</span>( Taps <span class="op">~</span><span class="st"> </span>Group, <span class="dt">data=</span>CaffeineTaps, <span class="dt">var.equal=</span><span class="ot">TRUE</span>, <span class="dt">conf.level=</span>.<span class="dv">99</span> ) </a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Taps by Group
## t = -3.3942, df = 18, p-value = 0.003233
## alternative hypothesis: true difference in means is not equal to 0
## 99 percent confidence interval:
##  -6.4681918 -0.5318082
## sample estimates:
## mean in group NoCaffeine   mean in group Caffeine 
##                    244.8                    248.3</code></pre>
</div>
</div>
<div id="additional-example" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Additional Example</h3>
<p><strong>Example:</strong> Does drinking beer increase your attractiveness to mosquitoes?</p>
<p>In places in the country substantial mosquito populations, the question of whether drinking beer causes the drinker to be more attractive to the mosquitoes than drinking something else has plagued campers. To answer such a question, researchers conducted a study to determine if drinking beer attracts more mosquitoes than drinking water. Of <span class="math inline">\(n=43\)</span> subjects, <span class="math inline">\(n_{b}=25\)</span> drank a liter beer and <span class="math inline">\(n_{w}=18\)</span> drank a liter of water and mosquitoes were caught in traps as they approached the different subjects. The critical part of this study is that the treatment (beer or water) was randomly assigned to each subject.</p>
<p>For this study, we want to test
<span class="math display">\[H_{0}:\:\delta=0\;\;\;\;\;\;\textrm{vs}\;\;\;\;\;\;H_{a}:\,\delta&lt;0\]</span>
where we define <span class="math inline">\(\delta=\mu_{w}-\mu_{b}\)</span> and <span class="math inline">\(\mu_{b}\)</span> is the mean number of mosquitoes attracted to a beer drinker and <span class="math inline">\(\mu_{w}\)</span> is the mean number attracted to a water drinker. As usual we begin our analysis by plotting the data.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" title="1"><span class="co"># I can&#39;t find this dataset on-line so I&#39;ll just type it in.</span></a>
<a class="sourceLine" id="cb187-2" title="2">Mosquitoes &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb187-3" title="3">  <span class="dt">Number =</span> <span class="kw">c</span>(<span class="dv">27</span>,<span class="dv">19</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">23</span>,<span class="dv">17</span>,<span class="dv">21</span>,<span class="dv">24</span>,<span class="dv">31</span>,<span class="dv">26</span>,<span class="dv">28</span>,<span class="dv">20</span>,<span class="dv">27</span>,</a>
<a class="sourceLine" id="cb187-4" title="4">             <span class="dv">19</span>,<span class="dv">25</span>,<span class="dv">31</span>,<span class="dv">24</span>,<span class="dv">28</span>,<span class="dv">24</span>,<span class="dv">29</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">18</span>,<span class="dv">27</span>,<span class="dv">20</span>,</a>
<a class="sourceLine" id="cb187-5" title="5">             <span class="dv">21</span>,<span class="dv">19</span>,<span class="dv">13</span>,<span class="dv">22</span>,<span class="dv">15</span>,<span class="dv">22</span>,<span class="dv">15</span>,<span class="dv">22</span>,<span class="dv">20</span>,</a>
<a class="sourceLine" id="cb187-6" title="6">             <span class="dv">12</span>,<span class="dv">24</span>,<span class="dv">24</span>,<span class="dv">21</span>,<span class="dv">19</span>,<span class="dv">18</span>,<span class="dv">16</span>,<span class="dv">23</span>,<span class="dv">20</span>),</a>
<a class="sourceLine" id="cb187-7" title="7">  <span class="dt">Treat =</span> <span class="kw">c</span>( <span class="kw">rep</span>(<span class="st">&#39;Beer&#39;</span>, <span class="dv">25</span>), <span class="kw">rep</span>(<span class="st">&#39;Water&#39;</span>,<span class="dv">18</span>) ) )</a>
<a class="sourceLine" id="cb187-8" title="8"></a>
<a class="sourceLine" id="cb187-9" title="9"><span class="co"># Plot the data</span></a>
<a class="sourceLine" id="cb187-10" title="10"><span class="kw">ggplot</span>(Mosquitoes, <span class="kw">aes</span>(<span class="dt">x=</span>Number)) <span class="op">+</span></a>
<a class="sourceLine" id="cb187-11" title="11"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb187-12" title="12"><span class="st">  </span><span class="kw">facet_grid</span>( Treat <span class="op">~</span><span class="st"> </span>. )</a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>For this experiment and the summary statistic that captures the difference we are trying to understand is <span class="math inline">\(d=\bar{x}_{w}-\bar{x}_{b}\)</span> where <span class="math inline">\(\bar{x}_{w}\)</span> is the sample mean number of mosquitoes attracted by the water group and <span class="math inline">\(\bar{x}_{b}\)</span> is the sample mean number of mosquitoes attracted by the beer group. Because of the order we chose for the subtraction, a negative value for d is supportive of the alternative hypothesis that mosquitoes are more attracted to beer drinkers.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" title="1">Mosquitoes <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(Treat) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb188-2" title="2"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">xbar.i =</span> <span class="kw">mean</span>(Number),</a>
<a class="sourceLine" id="cb188-3" title="3">            <span class="dt">s2.i   =</span> <span class="kw">var</span>(Number),</a>
<a class="sourceLine" id="cb188-4" title="4">            <span class="dt">s.i    =</span> <span class="kw">sd</span>(Number),</a>
<a class="sourceLine" id="cb188-5" title="5">            <span class="dt">n.i    =</span> <span class="kw">n</span>())</a></code></pre></div>
<pre><code>## # A tibble: 2 x 5
##   Treat xbar.i  s2.i   s.i   n.i
##   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1 Beer    23.6  17.1  4.13    25
## 2 Water   19.2  13.5  3.67    18</code></pre>
<p>Here we see that our statistic of interest is
<span class="math display">\[\begin{aligned} d 
  &amp;=    \bar{x}_{w}-\bar{x}_{b} \\
    &amp;=  19.22-23.6              \\
    &amp;=  -4.37\bar{7}
    \end{aligned}\]</span></p>
<p>We can use our numerical methods to evaluate statistical significance. First we perform the hypothesis test by creating the sampling distribution of <span class="math inline">\(d^*\)</span> assuming <span class="math inline">\(H_0\)</span> is true by repeatedly shuffling the group labels and calculating differences. We use <code>coin</code> to simplify the work. To make sure all of our calculations will match, we relevel so that <code>Water</code> is our first sample.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" title="1"><span class="co">### library(coin) required</span></a>
<a class="sourceLine" id="cb190-2" title="2">Mosquitoes<span class="op">$</span>Treat &lt;-<span class="st"> </span><span class="kw">relevel</span>(Mosquitoes<span class="op">$</span>Treat, <span class="st">&#39;Water&#39;</span>)</a>
<a class="sourceLine" id="cb190-3" title="3"><span class="kw">oneway_test</span>(Number<span class="op">~</span>Treat, <span class="dt">data=</span>Mosquitoes, <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>,</a>
<a class="sourceLine" id="cb190-4" title="4">            <span class="dt">distribution=</span><span class="kw">approximate</span>(<span class="dt">nresample=</span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>))</a></code></pre></div>
<pre><code>## 
##  Approximative Two-Sample Fisher-Pitman Permutation Test
## 
## data:  Number by Treat (Water, Beer)
## Z = -3.1673, p-value = 4e-04
## alternative hypothesis: true mu is less than 0</code></pre>
<p>From 10000 permutations, we obtain a p-value estimate of <span class="math inline">\(0.0004\)</span>.</p>
<p>The associated confidence interval (lets do a <span class="math inline">\(90\%\)</span> confidence level), is created via bootstrapping. The <code>diff.mean.func</code> was defined earlier in the chapter. Our data is in the form we need it, so we can run the bootstrap with the same setup as earlier.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" title="1">BootDist &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> Mosquitoes, <span class="dt">statistic =</span> diff.mean.function, <span class="dt">R=</span><span class="dv">10000</span>)</a></code></pre></div>
<p>We can visualize the distribution of the difference in means.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" title="1">BootDist.graph &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">dbar=</span>BootDist<span class="op">$</span>t)</a>
<a class="sourceLine" id="cb193-2" title="2"><span class="kw">ggplot</span>(BootDist.graph, <span class="kw">aes</span>(<span class="dt">x=</span>dbar)) <span class="op">+</span></a>
<a class="sourceLine" id="cb193-3" title="3"><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb193-4" title="4"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Difference in mean number of Mosquitoes between Water and Beer&#39;</span>)</a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>We can then extract the quantile-based 90% confidence interval.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" title="1">CI &lt;-<span class="st"> </span><span class="kw">quantile</span>( BootDist<span class="op">$</span>t, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>) )</a>
<a class="sourceLine" id="cb194-2" title="2">CI</a></code></pre></div>
<pre><code>##        5%       95% 
## -6.321483 -2.489701</code></pre>
<p>The calculated p-value is extremely small and the associated two-sided 90% confidence interval does not contain 0, so we can conclude at 10% significance that the choice of drink does cause a change in attractiveness to mosquitoes.</p>
<p>If we wanted to perform the same analysis using asymptotic methods we could do the calculations by hand, or just use R.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" title="1"><span class="kw">t.test</span>( Number <span class="op">~</span><span class="st"> </span>Treat, <span class="dt">data=</span>Mosquitoes, </a>
<a class="sourceLine" id="cb196-2" title="2">        <span class="dt">var.equal=</span><span class="ot">TRUE</span>, <span class="dt">conf.level=</span><span class="fl">0.90</span>)</a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Number by Treat
## t = -3.587, df = 41, p-value = 0.0008831
## alternative hypothesis: true difference in means is not equal to 0
## 90 percent confidence interval:
##  -6.431666 -2.323889
## sample estimates:
## mean in group Water  mean in group Beer 
##            19.22222            23.60000</code></pre>
<p>Because we releveled the groups to make <code>Water</code> first, this calculation matches everything demonstrated above.</p>
</div>
</div>
<div id="difference-in-means-between-two-groups-paired-data" class="section level2">
<h2><span class="header-section-number">7.2</span> Difference in means between two groups: Paired Data</h2>
<p>If the context of study is such that we can logically pair an observation from the first population to a particular observation in the second, then we can perform what is called a Paired Test. In a paired test, we will take each set of paired observations, calculate the difference, and then perform a 1-sample regular hypothesis test on the differences.</p>
<p>For example, in the package Lock5Data there is a dataset that examines the age in which men and women get married. The data was obtained by taking a random sample from publicly available marriage licenses in St. Lawrence County, NY.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb198-1" title="1"><span class="kw">data</span>(MarriageAges, <span class="dt">package=</span><span class="st">&#39;Lock5Data&#39;</span>)</a>
<a class="sourceLine" id="cb198-2" title="2"><span class="kw">head</span>(MarriageAges)</a></code></pre></div>
<pre><code>##   Husband Wife
## 1      53   50
## 2      38   34
## 3      46   44
## 4      30   36
## 5      31   23
## 6      26   31</code></pre>
<p>Unfortunately the format of this dataset is not particularly convenient for making graphs. Instead I want to turn this data into a “long” dataset where I have one row per person, not one row per marriage. We will also want to retain the groupings, so another column is created with this information (<code>Marriage</code>).</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" title="1"><span class="co"># Make a dataset that is more convenient for graphing.</span></a>
<a class="sourceLine" id="cb200-2" title="2">MarriageAges.Long &lt;-<span class="st"> </span>MarriageAges <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb200-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Marriage =</span> <span class="kw">factor</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">n</span>())) <span class="op">%&gt;%</span><span class="st">        </span><span class="co"># Give each row a unique ID </span></a>
<a class="sourceLine" id="cb200-4" title="4"><span class="st">  </span><span class="kw">gather</span>(<span class="st">&#39;Spouse&#39;</span>, <span class="st">&#39;Age&#39;</span>, Husband, Wife) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># pivot from Husband/Wife to Spouse/Age</span></a>
<a class="sourceLine" id="cb200-5" title="5"><span class="st">  </span><span class="kw">arrange</span>(Marriage, <span class="kw">desc</span>(Spouse))             <span class="co"># Sort by Marriage, then (Wife,Husband)</span></a>
<a class="sourceLine" id="cb200-6" title="6"><span class="kw">head</span>(MarriageAges.Long)</a></code></pre></div>
<pre><code>##   Marriage  Spouse Age
## 1        1    Wife  50
## 2        1 Husband  53
## 3        2    Wife  34
## 4        2 Husband  38
## 5        3    Wife  44
## 6        3 Husband  46</code></pre>
<p>We can then visualize the ages for each type of <code>Spouse</code>.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" title="1"><span class="co"># Make a graph of ages, by Spouse Type</span></a>
<a class="sourceLine" id="cb202-2" title="2"><span class="kw">ggplot</span>(MarriageAges.Long, <span class="kw">aes</span>(<span class="dt">x=</span>Age)) <span class="op">+</span></a>
<a class="sourceLine" id="cb202-3" title="3"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="dv">5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb202-4" title="4"><span class="st">  </span><span class="kw">facet_grid</span>(Spouse <span class="op">~</span><span class="st"> </span>.) </a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>Looking at this view of the data, it doesn’t appear that the husbands tend to be older than the wives. A t-test to see if the average age of husbands is greater than the average age of wives gives an insignificant difference. We will want to test if</p>
<p><span class="math display">\[\begin{aligned}
H_{0}:  &amp;\mu_{h}-\mu_{w}  =  0    \\
H_{a}:  &amp;\mu_{h}-\mu_{w} &gt; 0 
\end{aligned}\]</span></p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb203-1" title="1"><span class="kw">t.test</span>( Age <span class="op">~</span><span class="st"> </span>Spouse, <span class="dt">data=</span>MarriageAges.Long, <span class="dt">alternative=</span><span class="st">&#39;greater&#39;</span> )</a></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  Age by Spouse
## t = 1.8055, df = 203.12, p-value = 0.03624
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.2398733       Inf
## sample estimates:
## mean in group Husband    mean in group Wife 
##              34.66667              31.83810</code></pre>
<p>But critically, we are ignoring that while the average ages might not be different, for a given marriage, the husband tends to be older than the wife. Instead of looking at the difference in the means (i.e <span class="math inline">\(d=\bar{h}-\bar{w}\)</span>) we should actually be looking at the mean of the differences <span class="math inline">\(\bar{d}=\frac{1}{n}\sum d_{i}\)</span> where <span class="math inline">\(d_{i}=h_{i}-w_{i}\)</span>.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb205-1" title="1">MarriageAges &lt;-<span class="st"> </span>MarriageAges  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb205-2" title="2"><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">d =</span> Husband <span class="op">-</span><span class="st"> </span>Wife )   </a>
<a class="sourceLine" id="cb205-3" title="3"></a>
<a class="sourceLine" id="cb205-4" title="4"><span class="kw">ggplot</span>(MarriageAges, <span class="kw">aes</span>(<span class="dt">x =</span> d)) <span class="op">+</span></a>
<a class="sourceLine" id="cb205-5" title="5"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>Given this set of differences, we’d like to know if this data is compatible with the null hypothesis that husbands and wives tend to be the same age versus the alternative that husbands tend to be older. (We could chose the two-sided test as well).
<span class="math display">\[\begin{aligned}
H_{0}:\;\delta  &amp;=  0  \\
H_{A}:\;\delta  &amp;&gt;  0
\end{aligned}\]</span></p>
<p>Because we have reduced our problem to a 1-sample test, we can perform the asymptotic t-test easily enough in R. Notice now we are testing against the null hypothesis that <span class="math inline">\(\delta = 0\)</span>.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb206-1" title="1"><span class="kw">t.test</span>( MarriageAges<span class="op">$</span>d, <span class="dt">mu=</span><span class="dv">0</span> )</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  MarriageAges$d
## t = 5.8025, df = 104, p-value = 7.121e-08
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  1.861895 3.795248
## sample estimates:
## mean of x 
##  2.828571</code></pre>
<p>The result is highly statistically significant, and we see the mean difference in ages for the husband to be 2.8 years older.</p>
<p>To perform the same analysis using re-sampling methods, we need to be careful to do the re-sampling correctly. Notice that if we use <code>coin</code> how we set it up before, that we get something similar to when we were working under the assumption that the two groups were independent. The <code>coin</code> package requires objects be factors (the data.frame has it contained as a character or <code>chr</code>).</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb208-1" title="1"><span class="kw">oneway_test</span>(Age<span class="op">~</span><span class="kw">factor</span>(Spouse), <span class="dt">data=</span>MarriageAges.Long, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="dt">distribution=</span><span class="kw">approximate</span>(<span class="dt">nresample=</span><span class="dv">10</span><span class="op">^</span><span class="dv">4-1</span>))</a></code></pre></div>
<pre><code>## 
##  Approximative Two-Sample Fisher-Pitman Permutation Test
## 
## data:  Age by factor(Spouse) (Husband, Wife)
## Z = 1.7958, p-value = 0.0375
## alternative hypothesis: true mu is greater than 0</code></pre>
<p>The issue is that the permutations were done without taking into account that the Spouses are paired. The permutation test must be updated such that the paired nature of the marriages is taken into account. We can introduce this pairing using a conditional statement, where we want to ensure that we group the <code>Spouse</code> variable given the marriage they are in <code>Marriage</code>.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb210-1" title="1"><span class="kw">oneway_test</span>(Age<span class="op">~</span><span class="kw">factor</span>(Spouse)<span class="op">|</span><span class="kw">factor</span>(Marriage), <span class="dt">data=</span>MarriageAges.Long, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="dt">distribution=</span><span class="kw">approximate</span>(<span class="dt">nresample=</span><span class="dv">10</span><span class="op">^</span><span class="dv">4-1</span>))</a></code></pre></div>
<pre><code>## 
##  Approximative Two-Sample Fisher-Pitman Permutation Test
## 
## data:  Age by
##   factor(Spouse) (Husband, Wife) 
##   stratified by factor(Marriage)
## Z = 5.0675, p-value &lt; 1e-04
## alternative hypothesis: true mu is greater than 0</code></pre>
<p>After 10000 permutations, no mean difference in age was ever observed as extreme as the original and can only state that p-value <span class="math inline">\(&lt; 1e-4\)</span>. This is in agreement with the t.test performed above on the difference in ages for each marriage.</p>
<p>Finally, we can also perform bootstrap analysis. This now only requires that we use our bootstrap method from Chapter 3, as we only need to bootstrap the differences. We have not introduced <code>mean.function</code> so I must provide it now. I then run the bootstrap on the difference in ages for each marriage.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" title="1">mean.function &lt;-<span class="st"> </span><span class="cf">function</span>(x, index) {</a>
<a class="sourceLine" id="cb212-2" title="2">  d &lt;-<span class="st"> </span>x[index]</a>
<a class="sourceLine" id="cb212-3" title="3">  <span class="kw">return</span>(<span class="kw">mean</span>(d))  }</a>
<a class="sourceLine" id="cb212-4" title="4"></a>
<a class="sourceLine" id="cb212-5" title="5">BootDist &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> MarriageAges<span class="op">$</span>d, <span class="dt">statistic =</span> mean.function, <span class="dt">R=</span><span class="dv">10000</span>)</a></code></pre></div>
<p>I omit the visualization, but give the resulting 95% confidence interval.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb213-1" title="1"><span class="kw">quantile</span>( BootDist<span class="op">$</span>t, <span class="dt">probs=</span><span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>) )</a></code></pre></div>
<pre><code>##     2.5%    97.5% 
## 1.904762 3.790476</code></pre>
<p>We observe a similar p-value and confidence interval as we did using the asymptotic test as expected. We now have a variety of tests and conditions, and can perform the analysis under asymptotic assumptions or through numerical strategies.</p>
<div id="additional-example-1" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Additional Example</h3>
<p><strong>Example:</strong> Traffic Flow</p>
<p>Engineers in Dresden, Germany were looking at ways to improve traffic flow by enabling traffic lights to communicate information about traffic flow with nearby traffic lights and modify their timing sequence appropriately. The engineers wished to compare new flexible timing system with the standard fixed timing sequence by evaluating the delay at a randomly selected <span class="math inline">\(n=24\)</span> intersections in Dresden. The data show results of one experiment where they simulated buses moving along a street and recorded the delay time for both systems. Because each simulation is extremely intensive, they only simulated <span class="math inline">\(n=24\)</span> intersections instead of simulating the whole city.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" title="1"><span class="kw">data</span>(TrafficFlow, <span class="dt">package=</span><span class="st">&#39;Lock5Data&#39;</span>)  </a>
<a class="sourceLine" id="cb215-2" title="2"><span class="kw">head</span>(TrafficFlow)</a></code></pre></div>
<pre><code>##   Timed Flexible Difference
## 1    88       45         43
## 2    90       46         44
## 3    91       45         46
## 4    99       51         48
## 5   101       48         53
## 6   101       48         53</code></pre>
<p>We change the shape of the data to make it easier to work with.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" title="1"><span class="co"># A data set more convenient for Graphing and Permutation Tests.</span></a>
<a class="sourceLine" id="cb217-2" title="2">TrafficFlow.Long &lt;-<span class="st"> </span>TrafficFlow           <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb217-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Light =</span> <span class="kw">factor</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">n</span>()))           <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># Give each row a unique ID </span></a>
<a class="sourceLine" id="cb217-4" title="4"><span class="st">  </span><span class="kw">gather</span>(<span class="st">&#39;Seq&#39;</span>, <span class="st">&#39;Delay&#39;</span>, Flexible, Timed) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># pivot to SequenceType and Delay amount</span></a>
<a class="sourceLine" id="cb217-5" title="5"><span class="st">  </span><span class="kw">arrange</span>(Light, Seq)                         <span class="co"># Sort by Light, then by SequenceType</span></a>
<a class="sourceLine" id="cb217-6" title="6"><span class="kw">head</span>(TrafficFlow.Long)</a></code></pre></div>
<pre><code>##   Difference Light      Seq Delay
## 1         43     1 Flexible    45
## 2         43     1    Timed    88
## 3         44     2 Flexible    46
## 4         44     2    Timed    90
## 5         46     3 Flexible    45
## 6         46     3    Timed    91</code></pre>
<p>As usual, we’ll first examine the data with a graph.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" title="1"><span class="kw">ggplot</span>(TrafficFlow.Long, <span class="kw">aes</span>(<span class="dt">x=</span>Delay)) <span class="op">+</span></a>
<a class="sourceLine" id="cb219-2" title="2"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="dv">2</span>) <span class="op">+</span><span class="st">              </span><span class="co"># histograms of Delay time</span></a>
<a class="sourceLine" id="cb219-3" title="3"><span class="st">  </span><span class="kw">facet_grid</span>(Seq <span class="op">~</span><span class="st"> </span>.)                       <span class="co"># two plots, stacked by SequenceType</span></a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" title="1"><span class="kw">ggplot</span>(TrafficFlow, <span class="kw">aes</span>(<span class="dt">x=</span>Difference)) <span class="op">+</span></a>
<a class="sourceLine" id="cb220-2" title="2"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb220-3" title="3"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Difference (Standard - Flexible)&#39;</span>)</a></code></pre></div>
<p><img src="07_Two_Samples_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p>All of the differences were positive, so it is almost ridiculous to do a hypothesis test that there is no decrease in delays with the flexible timing system. We continue through the analysis. We begin with the asymptotic results, using the paired differences.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" title="1"><span class="kw">t.test</span>( TrafficFlow<span class="op">$</span>Difference )</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  TrafficFlow$Difference
## t = 19.675, df = 23, p-value = 6.909e-16
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  54.58639 67.41361
## sample estimates:
## mean of x 
##        61</code></pre>
<p>As expected, there is significant evidence that mean difference between Standard and Flexible. We can also run the permutation test under paired conditions. We are interested in the response <code>Delay</code> and how it is influenced by <code>Seq</code> the sequence time. We then also ensure that we properly pair the data, where are groupings are now the variable <code>Light</code>.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" title="1"><span class="kw">oneway_test</span>(Delay<span class="op">~</span><span class="kw">factor</span>(Seq)<span class="op">|</span><span class="kw">factor</span>(Light), <span class="dt">data=</span>TrafficFlow.Long, </a>
<a class="sourceLine" id="cb223-2" title="2">            <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>, <span class="dt">distribution=</span><span class="kw">approximate</span>(<span class="dt">nresample=</span><span class="dv">10</span><span class="op">^</span><span class="dv">4-1</span>))</a></code></pre></div>
<pre><code>## 
##  Approximative Two-Sample Fisher-Pitman Permutation Test
## 
## data:  Delay by
##   factor(Seq) (Flexible, Timed) 
##   stratified by factor(Light)
## Z = -4.7596, p-value &lt; 1e-04
## alternative hypothesis: true mu is not equal to 0</code></pre>
<p>We observe after 10000 iterations that again, no mean difference was ever as extreme as the original data set. Thus, we have p-value <span class="math inline">\(&lt; 1e^{-4}\)</span>. We finish with the bootstrap, performed on the differences.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" title="1">BootDist &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> TrafficFlow<span class="op">$</span>Difference, <span class="dt">statistic =</span> mean.function, <span class="dt">R=</span><span class="dv">10000</span>)</a></code></pre></div>
<p>I omit the visualization, but give the resulting 95% confidence interval.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" title="1"><span class="kw">quantile</span>( BootDist<span class="op">$</span>t, <span class="dt">probs=</span><span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>) )</a></code></pre></div>
<pre><code>##     2.5%    97.5% 
## 55.54167 67.50104</code></pre>
<p>The confidence interval suggests that these data support that the mean difference between the flexible timing sequence versus the standard fixed timing sequence in Dresden is in the interval <span class="math inline">\(\left(55.5,\,67.5\right)\)</span> seconds.</p>
</div>
</div>
<div id="exercises-6" class="section level2">
<h2><span class="header-section-number">7.3</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>In the 2011 article “Methane contamination of drinking water accompanying gas-well drilling and hydraulic fracturing” in the Proceedings of the National Academy of Sciences, <span class="math inline">\(n_{1}=21\)</span> sites in proximity to a fracking well had a mean methane level of <span class="math inline">\(\bar{x}_{1}=19.2\)</span> mg <span class="math inline">\(CH_{4} L^{-1}\)</span> with a sample standard deviation <span class="math inline">\(s_{1}=30.3\)</span>. The <span class="math inline">\(n_{2}=13\)</span> sites in the same region with no fracking wells within 1 kilometer had mean methane levels of <span class="math inline">\(\bar{x}_{2}=1.1\)</span> mg <span class="math inline">\(CH_{4} L^{-1}\)</span> and standard deviation <span class="math inline">\(s_{2}=6.3\)</span>. Perform a one-sided, two-sample t-test with unpooled variance and an <span class="math inline">\(\alpha=0.05\)</span> level to investigate if the presence of fracking wells increases the methane level in drinking-water wells in this region. Notice that because I don’t give you the data, you can only analyze the data using the asymptotic method and plugging in the give quantities into the formulas presented.
<ol style="list-style-type: lower-alpha">
<li>State an appropriate null and alternative hypothesis.</li>
<li>Calculate an appropriate test statistic (making sure to denote the appropriate degrees of freedom, if necessary).</li>
<li>Calculate an appropriate p-value.</li>
<li>At an significance level of <span class="math inline">\(\alpha=0.05\)</span>, do you reject or fail to reject the null hypothesis?</li>
<li>Restate your conclusion in terms of the problem.</li>
</ol></li>
<li>All persons running for public office must report the amount of money raised and spent during their campaign. Political scientists contend that it is more difficult for female candidates to raise money. Suppose that we randomly sample <span class="math inline">\(30\)</span> male and <span class="math inline">\(30\)</span> female candidates for state legislature and observe the male candidates raised, on average, <span class="math inline">\(\bar{y}=\$350,000\)</span> with a standard deviation of <span class="math inline">\(s_{y}=\$61,900\)</span> and the females raised on average <span class="math inline">\(\bar{x}=\$245,000\)</span> with a standard deviation of <span class="math inline">\(s_{x}=\$52,100\)</span>. Perform a one-sided, two-sample t-test with pooled variance to test if female candidates generally raise less in their campaigns that male candidates. <em>Notice that because I don’t give you the data, you can only analyze the data using the asymptotic method and plugging in the give quantities into the formulas presented.</em>
<ol style="list-style-type: lower-alpha">
<li>State an appropriate null and alternative hypothesis. (Be sure to use correct notation!)</li>
<li>Calculate an appropriate test statistic (making sure to denote the appropriate degrees of freedom, if necessary).</li>
<li>Calculate an appropriate p-value.</li>
<li>At an significance level of <span class="math inline">\(\alpha=0.05\)</span>, do you reject or fail to reject the null hypothesis?</li>
<li>Restate your conclusion in terms of the problem.</li>
</ol></li>
<li>In the Lock5Data package, the dataset <code>Smiles</code> gives data “…from a study examining the effect of a smile on the leniency of disciplinary action for wrongdoers. Participants in the experiment took on the role of members of a college disciplinary panel judging students accused of cheating. For each suspect, along with a description of the offense, a picture was provided with either a smile or neutral facial expression. Note, that for each individual only one picture was submitted. A leniency score was calculated based on the disciplinary decisions made by the participants.”
<ol style="list-style-type: lower-alpha">
<li>Graph the leniency score for the smiling and non-smiling groups. Comment on if you can visually detect any difference in leniency score.</li>
<li>Calculate the mean and standard deviation of the leniencies for each group. Does it seem reasonable that the standard deviation of each group is the same?</li>
<li>Do a two-sided two-sample t-test using pooled variance using the asymptotic method. Report the test statistic, p-value, and a <span class="math inline">\(95\%\)</span> CI.</li>
<li>Do a two-side two-sample t-test using re-sampling methods. Report the p-value and a <span class="math inline">\(95\%\)</span> CI.</li>
<li>What do you conclude at an <span class="math inline">\(\alpha=0.05\)</span> level? Do you feel we should have used a more stringent <span class="math inline">\(\alpha\)</span> level?</li>
</ol></li>
<li><p>In the Lock5Data package, the dataset <code>StorySpoilers</code> is data from an experiment where the researchers are testing if a “spoiler” at the beginning of a short story negatively affects the enjoyment of the story. A set of <span class="math inline">\(n=12\)</span> stories were selected and a spoiler introduction was created. Each version of each story was read by at least <span class="math inline">\(30\)</span> people and rated. Reported are the average ratings for the spoiler and non-spoiler versions of each story. The following code creates the “long” version of the data.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" title="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb228-2" title="2"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb228-3" title="3"><span class="kw">data</span>(StorySpoilers, <span class="dt">package=</span><span class="st">&#39;Lock5Data&#39;</span>)</a>
<a class="sourceLine" id="cb228-4" title="4">StorySpoilers.Long &lt;-<span class="st"> </span>StorySpoilers <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb228-5" title="5"><span class="st">  </span><span class="kw">gather</span>(<span class="st">&#39;Type&#39;</span>, <span class="st">&#39;Rating&#39;</span>, Spoiler, Original) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb228-6" title="6"><span class="st">  </span><span class="kw">mutate</span>( <span class="dt">Story =</span> <span class="kw">factor</span>(Story),      <span class="co"># make Story and Type into</span></a>
<a class="sourceLine" id="cb228-7" title="7">          <span class="dt">Type  =</span> <span class="kw">factor</span>(Type) ) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># categorical variables</span></a>
<a class="sourceLine" id="cb228-8" title="8"><span class="st">  </span><span class="kw">arrange</span>(Story)</a></code></pre></div>
<ol style="list-style-type: lower-alpha">
<li>Based on the description, a 1-sided test is appropriate. Explain why.</li>
<li>Graph the ratings for the original stories and the modified spoiler version. Comment on if you detect any difference in ratings between the two.</li>
<li>Graph the difference in ratings for each story. Comment on if the distribution of the differences seems to suggest that a spoiler lowers the rating.</li>
<li>Do a paired one-sided t-test using the asymptotic method. Also calculate a <span class="math inline">\(95\%\)</span> confidence interval.</li>
<li>Do a paired one-sided t-test using the permutation method. Also calculate a <span class="math inline">\(95\%\)</span> confidence interval using the bootstrap.</li>
<li>Based on your results in parts (d) and (e), what do you conclude?</li>
</ol></li>
<li><p>In the Lock5Data package, the dataset <code>Wetsuits</code> describes an experiment with the goal of quantifying the effect of wearing a wetsuit on the speed of swimming. (It is often debated among triathletes whether or not to wear a wetsuit when it is optional.) A set of <span class="math inline">\(n=12\)</span> swimmers and triathletes did a 1500 m swim in both the wetsuit and again in regular swimwear. The order in which they swam (wetsuit first or non-wetsuit first) was randomized for each participant. Reported is the maximum velocity during each swim.</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" title="1"><span class="co"># Code for creating the &quot;long&quot; version of the data</span></a>
<a class="sourceLine" id="cb229-2" title="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb229-3" title="3"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb229-4" title="4"><span class="kw">data</span>(<span class="st">&#39;Wetsuits&#39;</span>, <span class="dt">package=</span><span class="st">&#39;Lock5Data&#39;</span>)</a>
<a class="sourceLine" id="cb229-5" title="5">Wetsuits.Long &lt;-<span class="st"> </span>Wetsuits <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-6" title="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Participant =</span> <span class="kw">factor</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>) ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-7" title="7"><span class="st">  </span><span class="kw">gather</span>(<span class="st">&#39;Suit&#39;</span>, <span class="st">&#39;MaxVelocity&#39;</span>, Wetsuit,NoWetsuit) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-8" title="8"><span class="st">  </span><span class="kw">arrange</span>( Participant, Suit) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb229-9" title="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Suit =</span> <span class="kw">factor</span>(Suit))</a></code></pre></div>
<ol style="list-style-type: lower-alpha">
<li>Why did the researcher randomize which suit was worn first?</li>
<li>Plot the velocities for the wetsuit and non-wetsuit for each participant. Comment on if you detect any difference in the means of these two distributions.</li>
<li>Ignore the pairing and do a two-sided two-sample t-test using the asymptotic method. What would you conclude doing the t-test this way?</li>
<li>Plot the difference in velocity for each swimmer. Comment on if the observed difference in velocity seems to indicate that which should be preferred (wetsuit or non-wetsuit).</li>
<li>Do a paired two-sided t-test using the asymptotic method. Also calculate the 95% confidence interval. What do you conclude?</li>
<li>Do a paired two-sided t-test using the permutation method. Also calculate the 95% confidence interval using the bootstrap method. What do you conclude?</li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6-hypothesis-tests-for-the-mean-of-a-population.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix-a-alternative-bootstrap-code.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"],
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/570_II/raw/master/07_Two_Samples.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
