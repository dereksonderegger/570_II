[
["index.html", "Introduction to Statistical Methodology, Second Edition Preface Acknowledgements", " Introduction to Statistical Methodology, Second Edition Derek L. Sonderegger &amp; Robert Buscaglia September 11, 2019 Preface These notes were originally written for an introductory statistics course for grad students in the physical sciences. The problem with most introductory statistics courses is that they don’t prepare the student for the use of advanced statistics. Rote hand calculation is easy to test, easy to grade, and easy for students to learn to do, but is useless for actually understanding how to apply statistics. Since students pursuing a Ph.D. will likely be using statistics for the rest of their professional careers, we feel that this sort of course should attempt to steer away from a “cookbook” undergraduate pedagogy, and give the student enough theoretical background to continue their statistical studies at a high level while staying away from the painful mathematical details that statisticians must work through. Statistical software has progressed by leaps and bounds over the last decades. Scientists need access to reliable software that is flexible enough to handle new problems, with minimal headaches. R has become a widely used, and extremely robust Open Source platform for statistical computing and most new methodologies will appear in R before being incorporated into commercial software. Second, data exploration is the first step of any analysis and a user friendly yet powerful mechanism for graphing is a critical component in a researchers toolbox. R succeeds in this area with the most flexible graphing library of any statistical software and and basic plotting that can be executed quickly and easily. The only downside is that there is a substantial learning curve to scripting, particularly for students without any programming background. The use of R software is introduced with as little pain as possible, but some frustration is inevitable. Because the mathematical and statistical background of physical science students varies widely, the course seems to have a split-personality disorder. We wish to talk about using calculus to maximize the likelihood function and define the expectation of a continuous random variable, but also must spend time defining how to calculate a mean. We attempt to address both audiences, but recognize that it is not ideal. We hope you’ll find these notes useful. Acknowledgements Derek Sonderegger: I have had the pleasure of interacting with a great number of talented mathematicians and statisticians in my schooling. In particular I am deeply indebted to Dr Robert Boik and Dr Warren Esty as well as my Ph.D. adviser Dr Jan Hannig. As a professor at Northern Arizona University, I am grateful for the feedback and comradery of my fellow statisticians, particularly Dr St. Laurent and Dr. Buscaglia. Finally, I am deeply appreciative of the support given to me by my wife, Aubrey. Robert Buscaglia: I am thankful for Dr. Sonderegger allowing me to be a part of this online textbook. I would like to thank my graduate advisers, Dr. Jonathan B. Chaires and Dr. Yiannis Kamarianakis, who helped me achieve my goal of becoming a professor at Northern Arizona University. I would also be lost without the patience and kindness I receive from my family, especially my caring wife, Kelly. "],
["1-summary-statistics-and-graphing.html", "Chapter 1 Summary Statistics and Graphing 1.1 Variable Types 1.2 Randomization and Sampling 1.3 Graphical Summaries 1.4 Measures of Centrality 1.5 Measures of Spread 1.6 Shape 1.7 Exercises", " Chapter 1 Summary Statistics and Graphing When confronted with a large amount of data, we seek to summarize the data into statistics that capture the essence of the data with as few numbers as possible. Graphing the data has a similar goal: to reduce the data to an image that represents all the key aspects of the raw data. In short, we seek to simplify the data in order to understand the trends while not obscuring important structure. # Every chapter, we will load all the librarys we will use at the beginning # of the chapter. These commands will start most every homework assignment # for this class, and likely, every R script you write. library(ggplot2) # graphing functions library(dplyr) # data summary tools library(knitr) library(tidyr) # Set default behavior of ggplot2 graphs to be black/white theme theme_set(theme_bw()) For this chapter, we will consider data from a the 2005 Cherry Blossom 10 mile run that occurs in Washington DC. This data set has 8636 observations that includes the runners state of residence, official time (gun to finish, in seconds), net time (start line to finish, in seconds), age, and gender of the runners. data(TenMileRace, package=&#39;mosaicData&#39;) head(TenMileRace) # examine the first few rows of the data ## state time net age sex ## 1 VA 6060 5978 12 M ## 2 MD 4515 4457 13 M ## 3 VA 5026 4928 13 M ## 4 MD 4229 4229 14 M ## 5 MD 5293 5076 14 M ## 6 VA 6234 5968 14 M 1.1 Variable Types We will always want to be aware of the variable types in which we are working. We will distinguish variables into two principal categories: numerical and categorical. 1.1.1 Categorical Categorical variables are variables whose elements take on non-numerical entries. Examples within the TenMileRace set include the state and sex variables. Categorical variables are typically unordered, such that if we chose to order ‘NM’ before ‘AZ’ in an evaluation of the state variable, there would be no impact on our analysis. Categorical variables that have an implied order are termed ordinal variables. Examples include the common A, B, C, D, F grade-scale system. The variable entries are non-numerical, but there is an implied order that A &gt; B &gt; C &gt; D &gt; F. Such an ordering could influence the way the data is evaluated. 1.1.2 Numerical Numerical variables are broadly classified as variables with numerical elements. Numerical variables within the TenMileRace set include the time, net, and age variables. Numerical variables are sub-classified as either discrete or continuous. Discrete variables have entries that can be written as a list. Data that is discrete can take on a countable number of entries, such the variable age in years. We could write a list of numbers, {0, 1, 2, ..., 122}1, of which all values within the age variable could be drawn. Discrete variables are potentially finite, such as in the previous list for possible values of age. Additional examples include the number of students in a classroom or the number of offspring for a rabbit. Finite variables have important distributions such as the Binomial distribution. Discrete variables can also take on a potentially infinite number of possible values, but the values can still be listed, {0, 1, 2, ...}. Although there is no largest value within the list, the number of potential entries is still countable. Infinite valued discrete variables will also serve the basis for important distributions, such as the Poisson distribution. Continuous variables have entries that take on numerical values that lie on an interval. To decided if a data attribute is discrete or continuous, I often as “Does a fraction of a value make sense?” If so, then the data is continuous. The variables time and net are both recorded in seconds, and in this case seem to conform to discrete. However, if we had instead recorded the minutes with fractions of a minute present, such as 75.25 minutes instead of 4515 seconds, we might realize these variables are more likely to be considered continuous. Continuous variables constitute a large set of distributions that will be studied, the most commonly known being the Normal distribution. For the Normal distribution, it is possible to see values ranging from \\((-\\infty, \\infty)\\). This constitutes an interval, albeit a very large interval. Thus, elements of the variables lie on an interval, and it is not possible to list out all possible entries. Another simple example will be the Uniform distribution, whose entries lie on the interval (a,b), where a and b are any real valued number. Again, all potential observation of the variable can be found in the interval, but it is not possible to list out all possible outcomes. 1.2 Randomization and Sampling An important aspect of working within statistics is the concept that the data we are working with has been collected randomly. We think of having a population, the collection of all possible observations under consideration. The population will be dependent on the problem at hand. In the case of performing a study at a university, it may be that the entire university is the population. However, you may be working with only a specific subject, in which case the population may be only Mathematics majors. A sample is a subset of the population for which information is gathered. From the university example, we may choose to collect information from 1,000 students (our sample) which is drawn from the entire university population of 30,000 students (our population). The way we select our samples is done to ensure that we have randomly collected the data, such that there is no influence of correlation between samples interfering with our analysis. We will cover three broad sampling techniques that help ensure randomization of the samples collected. 1.2.1 Sampling Techniques Simple Random Sampling (SRS) is when every member of the population is equally-likely to be chosen. For SRS to be used, we also ensure that every member of the population is selected independently. Let us take the a university of having 30,000 students enrolled to be our population of which we would like to selected 1,000 as a sample. To use SRS, we would assign every member of the population a value {1, 2, ..., 30,000} and then draw numbers, without replacement, from our list of values. Such random numbers can be drawn using a random number generator, or traditionally through the use of a random number table. Below we show a simple method in R to draw 1,000 from 30,000 without replacement. sample(1:30000, 1000, replace=FALSE) Using this method would ensure that we obtain a random sampling drawn from the entire University. What we cannot do is draw a student, then also draw all of their siblings. If we were to use such a method, we would be introducing correlation within our samples. We must ensure that the students are all drawn randomly and that the selection is done independently. Random Cluster Sampling draws entire clusters based on a division of the population. In cluster sampling, the biggest idea is that we will draw entire clusters. Using the TenMileRace data, we could choose to create clusters from any of the variables. We could for example, cluster all participates based on state or create two clusters using sex, although two clusters may too limiting. We could also create ranges of values for the time, net, or age variables, and cluster the groups based on numerical ranges. Any of these methods would work for creating clusters. Let us consider clustering based on state. If one was to view this variable, they would find there are 62 unique state identifiers. This is due to there being several countries listed in this variable, as well as the inclusion of Washington, DC as its own state, and because it is real data, there is also one blank. The main concept though if we chose to cluster by state, we would produce 62 clusters, all of which are imbalanced in size. To complete cluster random sampling, we then use SRS to draw X states from the 62 clusters produced, such as say 10 from 62. This is done synonymously to the above method, but now from the 10 clusters chosen, we would sample ALL participants from within those clusters. Thus, if I were to draw the AZ cluster, I would sample all 3 participants. If I drew the VA cluster, we would sample all 3689 participants. Although this type of sampling is easier to produce larger samples with less randomization, we can see that clusters can be highly imbalanced, and it is unlikely that clustering will allow me to sub-sample from the entire population. Just in our example, I would not gather information from 52 of the 62 states, if I only was to draw 10 clusters. Stratified Sampling draws samples using proportionality based on homogeneous groupings known as strata. It is often easy to confuse Clustering and Stratified sampling, but the major difference here is that we will draw random samples from within the strata, unlike clustering where we take all individuals from the chosen clusters. Let us consider for exampling producing a random stratified sample using sex as our strata. Here, our homogeneous grouping is simply sex. Other examples might include stratifying animals by breed, stratifying the atmosphere by height above ground, or stratifying soil by depth. The main idea behind a strata is every member should be homogenized: in our example, we homogenized by ‘Male’ and ‘Female’. sex Frequency Proportion F 4325 0.501 M 4311 0.499 Above shows a table for the number of ‘Male’ and ‘Female’ participants. We see that these two strata are nearly equivalent, but we want to ensure we draw the samples based on proportionality. In total, we have 8636 participants. Let us say we want to draw \\(800\\) of these participants, but through stratification using sex. We must then ensure that when we draw a random sample, we obtain a sub-sample that has nearly equivalent proportions to that observed in the population. We must therefore draw \\(800*0.501 = 401\\) Males from the \\(4311\\) available and \\(800*0.499 = 399\\) Females from the \\(4325\\), where rounding was used. Notice this gives me \\(401 + 399 = 800\\) samples, and that I have \\(401/800 = 0.501\\) of the the sub-sample is Male and \\(399/800 = 0.499\\) is female. Thus, stratified sampling retains the proportions of the populations and allows me to sample from all strata. This can have desirable consequences, mainly that stratifying ensures samples are taken from all potential sources, here the sources are the different categories within our sex variable. Although unlikely, if I did draw samples using only SRS with no stratifying, I might get proportions of ‘Male’ and ‘Female’ that are close to that of the original sample. Stratifying guarantees we reproduce the proportions, while sampling from all homogeneous groupings. 1.3 Graphical Summaries 1.3.1 Barcharts/Barplots (Univariate - Categorical) If we have univariate data about a number of groups, often the best way to display it is using barplots. They have the advantage over pie-charts that groups are easily compared. The bars do NOT touch indicating that the order is not required, and the same information could be gained if we plotted them in a slightly different order. Below we compare the counts of ‘Male’ and ‘Female’ participants. ggplot(TenMileRace, aes(x=sex)) + geom_bar() One thing that can be misleading is if the zero on the y-axis is removed. In the following graph it looks like there are twice as many female runners as male until you examine the y-axis closely. In general, the following is a very misleading graph. ggplot(TenMileRace, aes(x=sex)) + geom_bar() + coord_cartesian(ylim = c(4300, 4330)) 1.3.2 Histogram (Univariate - Numerical) A histogram looks very similar to a bar plot, but is used to represent numerical data instead of categorical and therefore the bars will actually be touching. ggplot(TenMileRace, aes(x=net)) + geom_histogram() Often when a histogram is presented, the y-axis is labeled as “frequency” or “count” which is the number of observations that fall within a particular bin. However, it is often desirable to scale the y-axis so that if we were to sum up the area \\((height * width)\\) then the total area would sum to 1. The re-scaling that accomplishes this is \\[density=\\frac{\\#\\;observations\\;in\\;bin}{total\\;number\\;observations}\\cdot\\frac{1}{bin\\;width}\\] We can force the histogram created within ggplot to be display density by using the y=..density.. command. ggplot(TenMileRace, aes(x=net)) + geom_histogram(aes(y=..density..)) 1.3.3 Boxplot (Bivariate - Categorical vs Numerical) We often wish to compare response levels from two or more groups of interest. To do this, we often use side-by-side boxplots. Notice that each observation is associated with a continuous response value and a categorical value. ggplot(TenMileRace, aes(x=sex, y=net)) + geom_boxplot() In this graph, the edges of the box are defined by the 25% and 75% percentiles. That is to say, 25% of the data is to the below of the box, 50% of the data is in the box, and the final 25% of the data is to the above of the box. The line in the center of the box represents the 50% percentile, more commonly called the median. The dots are data points that are traditionally considered outliers. We will define the Inter-Quartile Range (IQR) as the length of the box. It is conventional to define any observation more than 1.5*IQR from the box as an outlier. In the above graph it is easy to see that the median time for the males is lower than for females, but the box width (one measure of the spread of the data) is approximately the same. Because boxplots simplify the distribution to just 5 numbers, looking at side-by-side histograms might give similar information. ggplot(TenMileRace, aes(x=net)) + geom_histogram() + facet_grid( . ~ sex ) # side-by-side plots based on sex Orientation of graphs can certainly matter. In this case, it makes sense to stack the two graphs to facilitate comparisons in where the centers are and it is more obvious that the center of the female distribution is about 500 to 600 seconds higher than then center of the male distribution. ggplot(TenMileRace, aes(x=net)) + geom_histogram() + facet_grid( sex ~ . ) # side-by-side plots based on sex 1.3.4 Scatterplot (Bivariate - Numerical vs Numerical) Finally we might want to examine the relationship between two numerical random variables. For example, we might wish to explore the relationship between a runners age and their net time. ggplot(TenMileRace, aes(x=age, y=net, color=sex)) + geom_point() 1.4 Measures of Centrality The most basic question to ask of any dataset is ‘What is the typical value?’ There are several ways to answer that question and they should be familiar to most students. 1.4.1 Mean Often called the average, or arithmetic mean, we will denote this special statistic with a bar. We define \\[\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}=\\frac{1}{n}\\left(x_{1}+x_{2}+\\dots+x_{n}\\right)\\] If we want to find the mean of five numbers \\(\\left\\{ 3,6,4,8,2\\right\\}\\) the calculation is \\[\\bar{x} = \\frac{1}{5}\\left(3+6+4+8+2\\right) = \\frac{1}{5}\\left(23\\right) = 23/5 = 4.6\\] This can easily be calculated in R by using the function mean(). We first extract the column we are interested in using the notation: DataSet$ColumnName where the $ signifies grabbing the column. mean( TenMileRace$net ) # Simplest way of doing this calculation ## [1] 5599.065 # Using the dplyr package we first specify the data set # Then specify we wish to summarize() the data set # The summary we want to do is to calculate the mean of the &#39;net&#39; column. # and we want to name what we are about to create as Calculated.Mean TenMileRace %&gt;% summarise( Calculated.Mean = mean(net) ) ## Calculated.Mean ## 1 5599.065 1.4.2 Median If the data were to be ordered, the median would be the middle most observation (or, in the case that \\(n\\) is even, the mean of the two middle most values). In our simple case of five observations \\(\\left\\{ 3,6,4,8,2\\right\\}\\), we first sort the data into \\(\\left\\{ 2,3,4,6,8\\right\\}\\) and then the middle observation is clearly \\(4\\). In R the median is easily calculated by the function median(). # median( TenMileRace$net ) TenMileRace %&gt;% summarise( Median = median(net) ) ## Median ## 1 5555 1.4.3 Mode This is peak in the distribution. A distribution might have a single peak or multiple peaks.This measure of “center” is not often used in quantitative analyses, but is often helps provide a nice description. When creating a histogram from a set of data, often the choice of binwidth will affect the modes of the graph. Consider the following graphs of \\(n=200\\) data points, where we have slightly different binwidths. With the two smaller binwidths, sample randomness between adjacent bins obscures the overall shape and we have many different modes. However the larger binwidth results in a histogram that more effectively communicates the shape of the distribution and has just a single mode at around 6000 seconds. When making histograms, the choice of binwidth (or equivalently, the number of bins) should not be ignored and a balance should be struck between simplifying the data too much vs seeing too much of the noise resulting from the sample randomness. 1.4.4 Examples Suppose a retired professor were to become bored and enroll in the author’s STA 570 course, how would that affect the mean and median age of the STA 570 students? The mean would move much more than the median. Suppose the class has 5 people right now, ages 21, 22, 23, 23, 24 and therefore the median is 23. When the retired professor joins, the ages will be 21, 22, 23, 23, 24, 72 and the median will remain 23. However, the mean would move because we add in such a large outlier. Whenever we are dealing with skewed data, the mean is pulled toward the outlying observations. In 2010, the median NFL player salary was $770,000 while the mean salary was $1.9 million. Why the difference? Because salary data is skewed by superstar players that make huge salaries (in excess of $20,000,000) while the minimum salary for a rookie is $375,000. Financial data often reflects a highly skewed distribution and the median is often a better measure of centrality in these cases. 1.5 Measures of Spread The second question to ask of a dataset is ‘How much spread is in the data?’ The fancier (and eventually more technical) word for spread is ‘variability’. As with centrality, there are several ways to measure this. 1.5.1 Range Range is the distance from the largest to the smallest value in the dataset. TenMileRace %&gt;% summarise( Range = max(net) - min(net) ) ## Range ## 1 7722 1.5.2 Inter-Quartile Range (IQR) The p-th percentile is the observation (or observations) that has at most \\(p\\) percent of the observations below it and \\((1-p)\\) above it, where \\(p\\) is between 0 and 100. The median is the \\(50\\)th percentile. Often we are interested in splitting the data into four equal sections using the \\(25\\)th, \\(50\\)th, and \\(75\\)th percentiles (which, because it splits the data into four sections, we often call these the \\(1\\)st, \\(2\\)nd, and \\(3\\)rd quartiles). In general we could be interested in dividing the data up into an arbitrary number of sections, and refer to those as quantiles of my data. quantile( TenMileRace$net ) # gives the 5-number summary by default ## 0% 25% 50% 75% 100% ## 2814 4950 5555 6169 10536 The IQR is defined as the distance between the \\(3\\)rd and \\(1\\)st quantiles. # IQR( TenMileRace$net ) TenMileRace %&gt;% summarise( CalcIQR = IQR(net) ) ## CalcIQR ## 1 1219 Notice that we’ve defined IQR before when we looked at boxplots, and that the IQR is exactly the length of the box. 1.5.3 Variance One way to measure the spread of a distribution is to ask “what is the typical distance of an observation to the mean?” We could define the \\(i\\)th deviation as \\[e_{i}=x_{i}-\\bar{x}\\] and then ask what is the average deviation? The problem with this approach is that the sum (and thus the average) of all deviations is always 0. \\[\\sum_{i=1}^{n}(x_{i}-\\bar{x}) = \\sum_{i=1}^{n}x_{i}-\\sum_{i=1}^{n}\\bar{x} = n\\frac{1}{n}\\sum_{i=1}^{n}x_{i}-n\\bar{x} = n\\bar{x}-n\\bar{x} = 0\\] The big problem is that about half the deviates are negative and the others are positive. What we really care is the distance from the mean, not the sign. So we could either take the absolute value, or square it. There are some really good theoretical reasons to chose the square option. Squared terms are easier to deal with computationally when compared to absolute values. More importantly, the spread of the normal distribution is parameterized via squared distances from the mean. Because the normal distribution is so important, we’ve chosen to define the sample variance so it matches up with the natural spread parameter of the normal distribution. So we square the deviations and find the average deviation size (approximately) and call that the sample variance. \\[s^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}\\] Why do we divide by \\(n-1\\) instead of \\(n\\)? If we divide by \\(n\\), then on average, we would tend to underestimate the population variance \\(\\sigma^{2}\\). The reason is because we are using the same set of data to estimate \\(\\sigma^{2}\\) as we did to estimate the population mean (\\(\\mu\\)). If we could use \\[\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}\\] as the estimator, we would be fine. But because we have to replace \\(\\mu\\) with \\(\\bar{x}\\) we have to pay a price. Because the estimation of \\(\\sigma^{2}\\) requires the estimation of one other quantity, we have used one degree of freedom on estimating the mean and we need to adjust the formula accordingly. In later chapters we’ll give this quantity a different name, so we’ll introduce the necessary vocabulary here. Let \\(e_{i}=x_{i}-\\bar{x}\\) be the error left after fitting the sample mean. This is the deviation from the observed value to the “expected value” \\(\\bar{x}\\). We can then define the Sum of Squared Error as \\[SSE=\\sum_{i=1}^{n}e_{i}^{2}\\] and the Mean Squared Error as \\[MSE=\\frac{SSE}{df}=\\frac{SSE}{n-1}=s^{2}\\] where \\(df=n-1\\) is the appropriate degrees of freedom. Calculating the variance of our small sample of five observations \\(\\left\\{ 3,6,4,8,2\\right\\}\\), recall that the sample mean was \\(\\bar{x}=4.6\\) \\(x_i\\) \\((x_i-\\bar{x})\\) \\((x_i-\\bar{x})^2\\) 3 -1.6 2.56 6 1.4 1.96 4 -0.6 0.36 8 3.4 11.56 2 -2.6 6.76 Sum = 0 SSE = 23.2 and so the sample variance is \\[s^2 = \\frac{SSE}{n-1} = \\frac{23.2}{(n-1)} = \\frac{23.2}{4}=5.8\\] Clearly this calculation would get very tedious to do by hand and computers will be much more accurate in these calculations. In R, the sample variance is easily calculated by the function var(). Given below is an example calculation done using dplyr commands. ToyData &lt;- data.frame( x=c(3,6,4,8,2) ) #var(ToyData$x) ToyData %&gt;% summarise( s2 = var(x) ) ## s2 ## 1 5.8 For the larger TenMileRace data set, the variance of the net time to complete the race is calculated just as easily. TenMileRace %&gt;% summarise( s2 = var(net) ) ## s2 ## 1 940233.5 1.5.4 Standard Deviation The biggest problem with the sample variance statistic is that the units are the original units-squared. That means if you are looking at data about car fuel efficiency, then the values would be in mpg\\(^{2}\\) which are units that I can’t really understand. The solution is to take the positive square root, which we will call the sample standard deviation. \\[s=\\sqrt{s^{2}}\\] Why do we take always evaluate variance? Mathematically the variance is more useful and most distributions (such as the normal) are defined by the variance term. Practically, standard deviation is easier to think about and becomes an informative quantity when discussing sample error. The sample standard deviation is important enough for R to have a function sd() that will calculate it for you. # sd( TenMileRace$net ) TenMileRace %&gt;% summarise( s = sd(net) ) ## s ## 1 969.6564 1.5.5 Coefficient of Variation Suppose we had a group of animals and the sample standard deviation of the animals lengths was 15 cm. If the animals were elephants, you would be amazed at their uniformity in size, but if they were insects, you would be astounded at the variability. To account for that, the coefficient of variation takes the sample standard deviation and divides by the absolute value of the sample mean (to keep everything positive) \\[CV=\\frac{s}{\\vert\\bar{x}\\vert}\\] Below is sample code to quickly grab the summary metrics of interest, with a calculation of the CV. TenMileRace %&gt;% summarise( s = sd(net), xbar = mean(net), CV = s / abs(xbar) ) ## s xbar CV ## 1 969.6564 5599.065 0.1731818 One final example showing how we can get information about grouped variables. Here, we would like to to calculate the same summary statistics as above, but would like to know them specificall for each factor with sex; that is, we want to compare ‘Male’ and ‘Female’. # Previously using dplyr notation didn&#39;t help too much, but if we wanted # to calculate the statistics separately for each sex, the dplyr solution # is MUCH easier. TenMileRace %&gt;% # Summarize the Ten Mile Race Data group_by(sex) %&gt;% # Group actions using sex summarise( xbar = mean(net), # s = sd(net), # cv = s / abs(xbar) ) # Calculate three different summary stats ## # A tibble: 2 x 4 ## sex xbar s cv ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F 5916. 902. 0.152 ## 2 M 5281. 930. 0.176 1.5.6 Empirical Rules For any data that are normally distributed (or approximately normal), the following are resourceful rules of thumb: Interval Approximate percent of Measurements \\(\\bar{x}\\pm s\\) 68% \\(\\bar{x}\\pm 2s\\) 95% \\(\\bar{x}\\pm 3s\\) 99.7% 1.6 Shape Vocabulary for discussing the shape of a distribution is discussed. These descriptors can be very useful for understanding the distribution, and as understanding develops, also tell us about relationships between the mean and median, or other informative quantities. 1.6.1 Symmetry A distribution is said to be symmetric if there is a point along the x-axis (which we’ll call \\(\\mu\\)) which acts as a mirror. Mathematically, a distribution is symmetric around \\(\\m\\) if and only if \\(f( -|x-\\mu| ) = f( |x-\\mu| )\\). The following graphs give the point of symmetry marked with a red line. A distribution that is not symmetric is said to be asymmetric. 1.6.2 Unimodal or Multi-modal Recall one measure of centrality was mode. If there is just a single mode, then we refer to the distribution as unimodal. If there is two or more we would refer to it as bimodal or multi-modal. 1.6.3 Skew If a distribution has a heavier tail on one side or the other, we refer to it as a skewed distribution and the direction of the skew is towards the heavier tail. Usually (but not always), an asymmetric distribution is skewed. 1.7 Exercises O&amp;L 3.21. The ratio of DDE (related to DDT) to PCB concentrations in bird eggs has been shown to have had a number of biological implications. The ratio is used as an indication of the movement of contamination through the food chain. The paper “The ratio of DDE to PCB concentrations in Great Lakes herring gull eggs and its us in interpreting contaminants data” reports the following ratios for eggs collected at 13 study sites from the five Great Lakes. The eggs were collected from both terrestrial and aquatic feeding birds. Source Type DDE to PCB Ratio Terrestrial 76.50, 6.03, 3.51, 9.96, 4.24, 7.74, 9.54, 41.70, 1.84, 2.5, 1.54 Aquatic 0.27, 0.61, 0.54, 0.14, 0.63, 0.23, 0.56, 0.48, 0.16, 0.18 By hand, compute the mean and median separately for each type of feeder. Using your results from part (a), comment on the relative sensitivity of the mean and median to extreme values in a data set. Which measure, mean or median, would you recommend as the most appropriate measure of the DDE to PCB level for both types of feeders? Explain your answer. O&amp;L 3.31. Consumer Reports in its June 1998 issue reports on the typical daily room rate at six luxury and nine budget hotels. The room rates are given in the following table. Hotel Type Nightly Rate Luxury $175, $180, $120, $150, $120, $125 Budget $50, $50, $49, $45, $36, $45, $50, $50, $40 By hand, compute the means and standard deviations of the room rates for each class of hotel. Give a practical reason why luxury hotels might have higher variability than the budget hotels. (Don’t just say the standard deviation is higher because there is more spread in the data, but rather think about the Hotel Industry and why you might see greater price variability for upscale goods compared to budget items.) Use R to confirm your calculations in problem 1 (the pollution data). Show the code you used and the subsequent output. It will often be convenient for me to give you code that generates a data frame instead of uploading an Excel file and having you read it in. The data can be generated using the following commands: PolutionRatios &lt;- data.frame( Ratio = c(76.50, 6.03, 3.51, 9.96, 4.24, 7.74, 9.54, 41.70, 1.84, 2.5, 1.54, 0.27, 0.61, 0.54, 0.14, 0.63, 0.23, 0.56, 0.48, 0.16, 0.18 ), Type = c( rep(&#39;Terrestrial&#39;,11), rep(&#39;Aquatic&#39;,10) ) ) head( PolutionRatios ) # Print out some data to confirm column names. ## Ratio Type ## 1 76.50 Terrestrial ## 2 6.03 Terrestrial ## 3 3.51 Terrestrial ## 4 9.96 Terrestrial ## 5 4.24 Terrestrial ## 6 7.74 Terrestrial Hint: for computing the means and medians for each type of feeder separately, the group_by() command we demonstated earlier in the chapter is convenient. Use R to confirm your calculations in problem 2 (the hotel data). Show the code you used and the subsequent output. The data can be loaded into a data frame using the following commands Show the code you used and the subsequent output: Hotels &lt;- data.frame( Price = c(175, 180, 120, 150, 120, 125, 50, 50, 49, 45, 36, 45, 50, 50, 40), Type = c( rep(&#39;Luxury&#39;,6), rep(&#39;Budget&#39;, 9) ) ) head( Hotels ) # Print out some data to confirm the column names. ## Price Type ## 1 175 Luxury ## 2 180 Luxury ## 3 120 Luxury ## 4 150 Luxury ## 5 120 Luxury ## 6 125 Luxury For the hotel data, create side-by-side box-and-whisker plots to compare the prices. Match the following histograms to the appropriate boxplot. Histogram A goes with boxplot __________ Histogram B goes with boxplot __________ Histogram C goes with boxplot __________ Histogram D goes with boxplot __________ Twenty-five employees of a corporation have a mean salary of $62,000 and the sample standard deviation of those salaries is $15,000. If each employee receives a bonus of $1,000, does the standard deviation of the salaries change? Explain your reasoning. The chemicals in clay used to make pottery can differ depending on the geographical region where the clay originated. Sometimes, archaeologists use a chemical analysis of clay to help identify where a piece of pottery originated. Such an analysis measures the amount of a chemical in the clay as a percent of the total weight of the piece of pottery. The boxplots below summarize analyses done for three chemicals—X, Y, and Z—on pieces of pottery that originated at one of three sites: I, II, or III. For chemical Z, describe how the percents found in the pieces of pottery are similar and how they differ among the three sites. Consider a piece of pottery known to have originated at one of the three sites, but the actual site is not known. Suppose an analysis of the clay reveals that the sum of the percents of the three chemicals X, Y, and Z is \\(20.5\\%\\). Based on the boxplots, which site—I, II, or III—is the most likely site where the piece of pottery originated? Justify your choice. Suppose only one chemical could be analyzed in the piece of pottery. Which chemical—X, Y, or Z— would be the most useful in identifying the site where the piece of pottery originated? Justify your choice. The efficacy of a new heart medication is being tested by evaluating its effect on a wide range of individuals. For each individual in the study the following characteristics are recorded prior to being given the medication: Gender, Ethnicity, Age (years), Height (m), Weight (kg), Blood Pressure (mmHg), Heart Rate (bpm). Determine the type of variable for each characteristic, briefly justify each answer. Grapes from a vineyard with 500 vines in Napa Valley are to be sampled. The investigator chooses to sample one grape from 100 different vines. What type of sampling is being done? Justify your response. R Experiment. Use the code below to generate 100 samples from a normal distribution. The normal distribution has a mean of 10 and a variance of 2. Be sure to include the set.seed function so all answers are the same. set.seed(10) rand.sample&lt;-rnorm(100, 10, 2) Use R to calculate the mean, median, variance, and IQR of rand.sample. Assign each value to variables with the names step1.mean, step1.median, step1.var, step1.IQR and have them output to the file. Do the mean and median calculated match the expected value of 10? Discuss why there may be discrepancies between the population mean and the sample mean. Next use the following code to augment rand.sample. This effectively adds two outliers to rand.sample. rand.sample.2&lt;-c(rand.sample, 250, 250) Use R to calculate the mean, median, variance, and IQR of rand.sample.2 and save them as variables named step2.mean, step2.median, step2.var, step2.IQR. Be sure to display all resulting summary statistics in the final RMD output. Discuss the differences in the statistics computed for rand.sample and rand.sample.2. Which statistics seem more resilient to the outliers? The oldest recorded age was that of a French women, Jeanne Calment, who lived to be to the age of 122 years.↩ "],
["2-probability.html", "Chapter 2 Probability 2.1 Introduction to Set Theory 2.2 Probability Rules 2.3 Discrete Random Variables 2.4 Common Discrete Distributions 2.5 Continuous Random Variables 2.6 R Quick Reference 2.7 Exercises", " Chapter 2 Probability # Every chapter, we will load all the librarys we will use at the beginning # of the chapter. library(ggplot2) # graphing functions library(dplyr) # data summary tools # Set default behavior of ggplot2 graphs to be black/white theme theme_set(theme_bw()) We need to work out the mathematics of what we mean by probability. To begin with we first define an outcome. An outcome is one observation from a random process or event. For example we might be interested in a single roll of a six-side die. Alternatively we might be interested in selecting one NAU student at random from the entire population of NAU students. 2.1 Introduction to Set Theory Before we jump into probability, it is useful to review a little bit of set theory. Events are properties of a particular outcome. For a coin flip, the event “Heads” would be the event that a heads was flipped. For the single roll of a six-sided die, a possible event might be that the result is even. For the NAU student, we might be interested in the event that the student is a biology student. A second event of interest might be if the student is an undergraduate. 1.1.1 Venn Diagrams Let \\(S\\) be the set of all outcomes of my random trial. Suppose I am interested in two events \\(A\\) and \\(B\\). The traditional way of representing these events is using a Venn diagram. For example, suppose that my random experiment is rolling a fair 6-sided die once. The possible outcomes are \\(S=\\{1,2,3,4,5,6\\}\\). Suppose I then define events \\(A=\\) roll is odd and \\(B=\\) roll is 5 or greater. In this case our picture is: All of our possible events are present, and distributed among our possible events. 2.1.1 Composition of events I am often interested in discussing the composition of two events and we give the common set operations below. Union: Denote the event that either \\(A\\) or \\(B\\) occurs as \\(A\\cup B\\). Intersection: Denote the event that both \\(A\\) and \\(B\\) occur as \\(A\\cap B\\) Complement: Denote the event that \\(A\\) does not occur as \\(\\bar{A}\\) or \\(A^{C}\\) (different people use different notations) Definition: Two events \\(A\\) and \\(B\\) are said to be mutually exclusive (or disjoint) if the occurrence of one event precludes the occurrence of the other. For example, on a single roll of a die, a two and a five cannot both come up. For a second example, define \\(A\\) to be the event that the die is even, and \\(B\\) to be the event that the die comes up as a \\(5\\). 2.2 Probability Rules 2.2.1 Simple Rules We now take our Venn diagrams and use them to understand the rules of probability. The underlying idea that we will use is the the probability of an event is the area in the Venn diagram. Definition: The probability is the proportion of times an event occurs in many repeated trials of a random phenomenon. In other words, it is the long-term relative frequency. Rule: For any event \\(A\\) the probability of the event \\(P(A)\\) satisfies \\(0\\leq P(A) \\leq 1\\). That is to say, the probability of any event will always lie in the interval \\([0,1]\\). Because \\(S\\) is the set of all events that might occur, the area of our bounding rectangle will be \\(1\\) and the probability of event \\(A\\) occurring will be represented by the area in the circle \\(A\\). Rule: The probability of the set of all events (\\(S\\)) is always 1. That is, \\(P(S) = 1\\). General Addition Rule: \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) The reason behind this fact is that if there is if \\(A\\) and \\(B\\) are not disjoint, then some area is added twice when I calculate \\(P\\left(A\\right)+P\\left(B\\right)\\). To account for this, I simply subtract off the area that was double counted. Rule: If two events are mutually exclusive, then \\(P(A\\cup B)=P(A)+P(B)\\) Example. Let \\(R\\) be the sum of two different colored dice. Suppose we are interested in \\(P(R \\le 4)\\). Notice that the pair of dice can fall 36 different ways (6 ways for the first die and six for the second results in 6x6 possible outcomes, and each way has equal probability \\(1/36\\). Because the dice cannot simultaneously sum to \\(2\\) and to \\(3\\), we could write \\[\\begin{aligned} P(R \\le 4 ) &amp;= P(R=2)+P(R=3)+P(R=4) \\\\ &amp;= P(\\left\\{ 1,1\\right\\} )+P(\\left\\{ 1,2\\right\\} \\mathrm{\\textrm{ or }}\\left\\{ 2,1\\right\\} )+P(\\{1,3\\}\\textrm{ or }\\{2,2\\}\\textrm{ or }\\{3,1\\}) \\\\ &amp;= \\frac{1}{36}+\\frac{2}{36}+\\frac{3}{36} \\\\ &amp;= \\frac{6}{36} \\\\ &amp;= \\frac{1}{6} \\end{aligned}\\] Complement Rule: \\(P(A)+P(A^c)=1\\) This rule follows from the partitioning of the set of all events (\\(S\\)) into two disjoint sets, \\(A\\) and \\(A^c\\). We learned above that \\(A \\cup A^c = S\\) and that \\(P(S) = 1\\). Combining those statements, we obtain the complement rule. Completeness Rule: \\(P(A)=P(A\\cap B)+P(A\\cap B^c)\\) This identity is just breaking the event \\(A\\) into two disjoint pieces. 2.2.2 Conditional Probability We are given the following data about insurance claims. Notice that the data is given as \\(P(\\;Category\\;\\cap\\;PolicyType\\;)\\) which is apparent because the sum of all the elements in the table is \\(100\\%\\) \\(\\,\\) Fire Auto Other Fraudulant 6% 1% 3% non-Fraudulant 14% 29% 47% Summing across the rows and columns, we can find the probabilities of for each category and policy type. \\(\\,\\) Fire Auto Other \\(\\,\\) Fraudulant 6% 1% 3% 10% non-Fraudulant 14% 29% 47% 90% \\(\\,\\) 20% 30% 50% 100% It is clear that fire claims are more likely fraudulent than auto or other claims. In fact, the proportion of fraudulent claims, given that the claim is against a fire policy is \\[\\begin{aligned} P(\\textrm{ Fraud }|\\textrm{ FirePolicy }) &amp;= \\frac{\\textrm{proportion of claims that are fire policies and are fraudulent}}{\\textrm{proportion of fire claims}} \\\\ &amp;= \\frac{6\\%}{20\\%}\\\\ &amp; \\\\ &amp;= 0.3 \\end{aligned}\\] In general we define conditional probability (assuming \\(P(B) \\ne 0\\)) as \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\] which can also be rearranged to show \\[\\begin{aligned} P(A\\cap B) &amp;= P(A\\,|\\,B)\\,P(B) \\\\ &amp;= P(B\\,|\\,A)\\,P(A) \\end{aligned}\\] Because the order doesn’t matter and \\(P\\left(A\\cap B\\right)=P\\left(B\\cap A\\right)\\). Using this rule, we might calculate the probability that a claim is an Auto policy given that it is not fraudulent. \\[\\begin{aligned} P\\left(\\,Auto\\;|\\;NotFraud\\,\\right) &amp;= \\frac{P\\left(\\,Auto\\;\\cap\\;NotFraud\\right)}{P\\left(\\,NotFraud\\,\\right)} \\\\ &amp;= \\frac{0.29}{0.9} \\\\ &amp; \\\\ &amp;= 0.3\\bar{2} \\end{aligned}\\] Definition: Two events \\(A\\) and \\(B\\) are said to be independent if \\(P(A\\cap B)=P(A)P(B)\\). What independence is saying that knowing the outcome of event \\(A\\) doesn’t give you any information about the outcome of event \\(B\\). Thus, we can use conditional statements to also show that two events are independent if \\(P(A|B) = P(A)\\). In simple random sampling, we assume that any two samples are independent. In cluster sampling, we assume that samples within a cluster are not independent, but clusters are independent of each other. Fact: If \\(A\\) and \\(B\\) are independent events, then \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\). These statements follow directly from the given definitions. Example: Suppose that we are interested in the relationship between the color and the type of car. Specifically I will divide the car world into convertibles and non-convertibles and the colors into red and non-red. Suppose that convertibles make up just 10% of the domestic automobile market. This is to say \\(P(\\;Convertable\\;)=0.10\\). Of the non-convertibles, red is not unheard of but it isn’t common either. So suppose \\(P(\\;Red\\;|\\;NonConvertable\\;)=0.15\\). However red is an extremely popular color for convertibles so let \\(P(\\;Red\\;|\\;Convertable\\;)=0.60\\). Given the above information, we can create the following table: \\(\\,\\) Convertible Not Convertible \\(\\,\\) Red Not Red \\(\\,\\) 10% 90% 100% We can fill in some of the table using our the definition of conditional probability. For example: \\[\\begin{aligned} P\\left(Red\\,\\cap\\,Convertable\\right) &amp;= P\\left(Red\\,|\\,Convertable\\right)\\,P\\left(Convertable\\right) \\\\ &amp;= 0.60*0.10 \\\\ &amp;= 0.06 \\end{aligned}\\] Lets think about what this conditional probability means. Of the \\(90\\%\\) of cars that are not convertibles, \\(15\\%\\) those non-convertibles are red and therefore the proportion of cars that are red non-convertibles is \\(0.90*0.15=0.135\\). Of the \\(10\\%\\) of cars that are convertibles, \\(60\\%\\) of those are red and therefore proportion of cars that are red convertibles is \\(0.10*0.60=0.06\\). Thus the total percentage of red cars is actually \\[\\begin{aligned}P\\left(\\,Red\\,\\right) &amp;= P\\left(\\;Red\\;\\cap\\;Convertible\\;\\right)+P\\left(\\,Red\\,\\cap\\,NonConvertible\\,\\right)\\\\ &amp;= P\\left(\\,Red\\,|\\,Convertable\\,\\right)P\\left(\\,Convertible\\,\\right)+P\\left(\\,Red\\,|\\,NonConvertible\\,\\right)P\\left(\\,NonConvertible\\,\\right)\\\\ &amp;= 0.60*0.10+0.15*0.90\\\\ &amp;= 0.06+0.135\\\\ &amp;= 0.195 \\end{aligned}\\] So when I ask for \\(P(\\;red\\;|\\;convertable\\;)\\), I am narrowing my space of cars to consider only convertibles. While there percentage of cars that are red and convertible is just 6% of all cars, when I restrict myself to convertibles, we see that the percentage of this smaller set of cars that are red is 60%. Notice that because \\(P\\left(Red\\right)=0.195\\ne0.60=P\\left(Red\\,|\\,Convertable\\right)\\) then the events \\(Red\\) and \\(Convertable\\) are not independent. 2.2.3 Summary of Probability Rules Here we give a short summary of the most frequently used rules. \\[0 \\le P\\left(A\\right) \\le 1\\] \\[P\\left(A\\right)+P\\left(A^c\\right)=1\\] \\[P\\left(A\\cup B\\right) = P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)\\] \\[P\\left(A\\cap B\\right) = \\begin{cases} P\\left(A\\,|\\,B\\right)P\\left(B\\right)\\\\ P\\left(B\\,|\\,A\\right)P\\left(A\\right)\\\\ P(A)P(B)\\;\\; &amp; \\textrm{ if A,B are independent} \\end{cases}\\] \\[P\\left(A\\,|\\,B\\right) = \\frac{P\\left(A\\cap B\\right)}{P\\left(B\\right)}\\] 2.3 Discrete Random Variables The different types of probability distributions (and therefore your analysis method) can be divided into two general classes: Continuous Random Variables - the variable takes on numerical values and could, in principle, take any of an uncountable number of values. In practical terms, if fractions or decimal points in the number make sense, it is usually continuous. Discrete Random Variables - the variable takes on one of small set of values (or only a countable number of outcomes). In practical terms, if fractions or decimals points don’t make sense, it is usually discrete. Examples: Presence or Absence of wolves in a State? Number of Speeding Tickets received? Tree girth (in cm)? Photosynthesis rate? 2.3.1 Introduction to Discrete Random Variables The following facts hold for discrete random variables: The probability associated with every value lies between 0 and 1 The sum of all probabilities for all values is equal to 1 Probabilities for discrete RVs are additive. i.e., \\(P(3\\textrm{ or }4)=P(3)+P(4)\\) 2.3.1.1 Expected Value Example: Consider the discrete random variable \\(S\\), the sum of two fair dice. We often want to ask ‘What is expected value of this distribution?’ You might think about taking a really, really large number of samples from this distribution and then taking the mean of that really really big sample. We define the expected value (often denoted by \\(\\mu\\)) as a weighted average of the possible values and the weights are the proportions with which those values occur. \\[\\mu=E[S] = \\sum_{\\textrm{possible }s}\\;s\\cdot P\\left(S=s\\right)\\] In this case, we have that \\[\\begin{aligned} \\mu = E[S] &amp;= \\sum_{s=2}^{12}s\\cdot P(S=s) \\\\ &amp;= 2\\cdot P\\left(S=2\\right)+3\\cdot P\\left(S=3\\right)+\\dots+11\\cdot P\\left(S=11\\right)+12\\cdot P\\left(S=12\\right) \\\\ &amp;= 2\\left(\\frac{1}{36}\\right)+3\\left(\\frac{2}{36}\\right)+\\dots+11\\left(\\frac{2}{36}\\right)+12\\left(\\frac{1}{36}\\right) \\\\ &amp;= 7 \\end{aligned}\\] 2.3.1.2 Variance Similarly we could define the variance of \\(S\\) (which we often denote \\(\\sigma^{2}\\)) as a weighted average of the squared-deviations that could occur. \\[ \\sigma^{2}=V[S] = \\sum_{\\textrm{possible }s}\\; (s-\\mu)^2 \\cdot P\\left(S=s\\right)\\] which in this example can be calculated as \\[\\begin{aligned} \\sigma^{2}=V[S] &amp;= \\sum_{s=2}^{12}\\left(s-\\mu\\right)^{2}P(S=s) \\\\ &amp;= (2-7)^{2}\\left(\\frac{1}{36}\\right)+(3-7)^{2}\\left(\\frac{2}{36}\\right)+\\dots+(12-7)^{2}\\left(\\frac{1}{36}\\right) \\\\ &amp;= \\frac{35}{6}=5.8\\bar{3} \\end{aligned}\\] We could interpret the expectation as the sample mean of an infinitely large sample, and the variance as the sample variance of the same infinitely large sample. These are two very important numbers that describe the distribution. Example: My wife is a massage therapist and over the last year, the number of clients she sees per work day (denoted Y) varied according the following table: Number of Clients 0 1 2 3 4 Frequency/Probability 0.30 0.35 0.20 0.10 0.05 distr &lt;- data.frame( clients = c( 0, 1, 2, 3, 4 ), # two columns probability = c(0.3, 0.35, 0.20, 0.10, 0.05 ) ) # ggplot(distr, aes(x=clients)) + # graph with clients as the x-axis geom_point(aes(y=probability)) + # where the dots go geom_linerange(aes(ymax=probability, ymin=0)) + # the vertical lines theme_bw() # set background color... Because this is the long term relative frequency of the number of clients (over 200 working days!), it is appropriate to interpret these frequencies as probabilities. This table and graph is often called a probability mass function (pmf) because it lists how the probability is spread across the possible values of the random variable. We might next ask ourselves what is the average number of clients per day? \\[\\begin{aligned} E\\left(Y\\right) &amp;= \\sum_{\\textrm{possible }y}y\\,P\\left(Y=y\\right) \\\\ &amp;= \\sum_{y=0}^{4}y\\,P\\left(Y=y\\right) \\\\ &amp;= 0\\,P\\left(Y=0\\right)+1\\,P\\left(Y=1\\right)+2\\,P\\left(Y=2\\right)+3\\,P\\left(Y=3\\right)+4\\,P\\left(Y=4\\right) \\\\ &amp;= 0\\left(0.3\\right)+1\\left(0.35\\right)+2\\left(0.20\\right)+3\\left(0.10\\right)+4\\left(0.05\\right) \\\\ &amp;= 1.25 \\end{aligned}\\] Notice that this number is not an integer and therefore is not a value that \\(Y\\) could actually take on. You might be tempted to therefore round it to the nearest integer. That would be wrong. The rational is that if we wanted to estimate the number of clients she has per month (and thus her income), we would have a worse estimate if we used the rounded number. Another example of a case where rounding would be inappropriate is in gambling situations where the amount won or lost per hand isn’t particularly important but the average amount won or lost over hundreds or thousands of plays is what matters. A Roulette wheel has 18 red and 18 black slots along with 2 green. If you bet $1 on red, you could either win a dollar or lose a dollar. However, because the probabilities are Win ( + $1 ) Lose (- $1) Probability \\(\\frac{18}{38}\\) \\(\\frac{20}{38}\\) then the persons expected winnings per play are: \\[ \\begin{aligned}E[W] = \\sum_{\\textrm{possible }w}w\\,P\\left(W=w\\right) = 1 \\left(\\frac{18}{38} \\right) + -1 \\left( \\frac{20}{38} \\right) = -0.0526 \\end{aligned}\\] So for every Black/Red bet, the player should expect to lose 5.2 cents. While this number is small, it is enough to make the casino millions of dollars over the long run. Returning to the massage therapy example, assuming that successive days are independent (which might be a bad assumption) what is the probability she has two days in a row with no clients? \\[\\begin{aligned}P\\left(\\textrm{0 on day1 }and\\textrm{ 0 on day2}\\right) &amp;= P\\left(\\textrm{0 on day 1}\\right)P\\left(\\textrm{0 on day 2}\\right) \\\\ &amp;= \\left(0.3\\right)\\left(0.3\\right) \\\\ &amp;= 0.09 \\end{aligned}\\] What is the variance of this distribution? \\[\\begin{aligned}V\\left(Y\\right) &amp;= \\sum_{\\textrm{possible y}}\\,\\left(y-\\mu\\right)^{2}\\,P\\left(Y=y\\right) \\\\ &amp;= \\sum_{y=0}^{4}\\,\\left(y-\\mu\\right)^{2}P\\left(Y=y\\right) \\\\ &amp;= \\left(0-1.25\\right)^{2}\\left(0.3\\right)+\\left(1-1.25\\right)^{2}\\left(0.35\\right)+\\left(2-1.25\\right)^{2}\\left(0.20\\right)+\\left(3-1.25\\right)^{2}\\left(0.10\\right)+\\left(4-1.25\\right)^{2}\\left(0.05\\right) \\\\ &amp;= 1.2875 \\end{aligned}\\] Note on Notation: There is a difference between the upper and lower case letters we have been using to denote a random variable. In general, we let the upper case denote the random variable and the lower case as a value that the the variable could possibly take on. So in the massage example, the number of clients seen per day \\(Y\\) could take on values \\(y=0,1,2,3,\\) or \\(4\\). 2.4 Common Discrete Distributions 2.4.1 Binomial Distribution Example: Suppose we are trapping small mammals in the desert and we spread out three traps. Assume that the traps are far enough apart that having one being filled doesn’t affect the probability of the others being filled and that all three traps have the same probability of being filled in an evening. Denote the event that a trap is filled with a critter as \\(C_{i}\\) and denote the event that the trap is empty as \\(E_{i}\\). Denote the probability that a trap is filled by \\(\\pi=0.8\\). (This sort of random variable is often referred to as a Bernoulli RV.) The possible outcomes are Outcome \\(\\,\\) \\(E_1, E_2, E_3\\) \\(\\,\\) \\(C_1, E_2, E_3\\) \\(\\,\\) \\(E_1, C_2, E_3\\) \\(\\,\\) \\(E_1, E_2, C_3\\) \\(\\,\\) \\(C_1, C_2, E_3\\) \\(\\,\\) \\(C_1, E_2, C_3\\) \\(\\,\\) \\(E_1, C_2, C_3\\) \\(\\,\\) \\(C_1, C_2, C_3\\) \\(\\,\\) Because these are far apart enough in space that the outcome of Trap1 is independent of Trap2 and Trap3, then \\[P(E_{1}\\cap C_{2}\\cap E_{3}) = P(E_{1})P(C_{2})P(E_{3}) = (1-0.8)0.8(1-0.8) = 0.032\\] Notice how important the assumption of independence is!!! Similarly we could calculate the probabilities for the rest of the table. Outcome Probability \\(S\\) Outcome Probability \\(E_1, E_2, E_3\\) 0.008 \\(S=0\\) 0.008 ——————- ————— ————- ————— \\(C_1, E_2, E_3\\) 0.032 \\(E_1, C_2, E_3\\) 0.032 \\(S=1\\) \\(3(0.032) = 0.096\\) \\(E_1, E_2, C_3\\) 0.032 ——————- ————— ————- ————— \\(C_1, C_2, E_3\\) 0.128 \\(C_1, E_2, C_3\\) 0.128 \\(S=2\\) \\(3(0.128) = 0.384\\) \\(E_1, C_2, C_3\\) 0.128 ——————- ————— ————- ————— \\(C_1, C_2, C_3\\) 0.512 \\(S=3\\) \\(0.512\\) Next we are interested in the random variable \\(S\\), the number of traps that were filled: \\(S\\) Outcome Probability \\(S=0\\) \\(0.008\\) \\(S=1\\) \\(0.096\\) \\(S=2\\) \\(0.384\\) \\(S=3\\) \\(0.512\\) \\(S\\) is an example of a Binomial Random Variable. A binomial experiment is one that: Experiment consists of \\(n\\) identical trials. Each trial results in one of two outcomes (Heads/Tails, presence/absence). One will be labeled a success and the other a failure. The probability of success on a single trial is equal to \\(\\pi\\) and remains the same from trial to trial. The trials are independent (this is implied from property 3). The random variable \\(Y\\) is the number of successes observed during \\(n\\) trials. Recall that the probability mass function (pmf) describes how the probability is spread across the possible outcomes, and in this case, I can describe this via a nice formula. The pmf of a a binomial random variable \\(X\\) taken from \\(n\\) trials each with probability of success \\(\\pi\\) is \\[P(X=x)=\\underbrace{\\frac{n!}{x!(n-x)!}}_{orderings}\\;\\underbrace{\\pi^{x}}_{y\\,successes}\\;\\underbrace{(1-\\pi)^{n-x}}_{n-y\\,failures}\\] where we define \\(n!=n(n-1)\\dots(2)(1)\\) and further define \\(0!=1\\). Often the ordering term is written more compactly as \\[{n \\choose x}=\\frac{n!}{x!\\left(n-x\\right)!}\\]. For our small mammal example we can create a graph that shows the binomial distribution with the following R code: dist &lt;- data.frame( x=0:3 ) %&gt;% mutate(probability = dbinom(x, size=3, prob=0.8)) ggplot(dist, aes(x=x)) + geom_point(aes(y=probability)) + geom_linerange(aes(ymax=probability, ymin=0)) + ggtitle(&#39;Binomial distribution: n=3, p=0.8&#39;) + theme_bw() To calculate the height of any of these bars, we can evaluate the pmf at the desired point. For example, to calculate the probability the number of full traps is 2, we calculate the following \\[\\begin{aligned} P(X=2) &amp;= {3 \\choose 2}\\left(0.8\\right)^{2}\\left(1-0.8\\right)^{3-2} \\\\ &amp;= \\frac{3!}{2!(3-2)!}(0.8)^{2}(0.2)^{3-2} \\\\ &amp;= \\frac{3\\cdot2\\cdot1}{(2\\cdot1)1}\\;(0.8)^{2}(0.2) \\\\ &amp;= 3(0.128) \\\\ &amp;= 0.384 \\end{aligned}\\] You can use R to calculate these probabilities. In general, for any distribution, the “d-function” gives the distribution function (pmf or pdf). So to get R to do the preceding calculation we use: # If X ~ Binomial(n=3, pi=0.8) # Then P( X = 2 | n=3, pi=0.8 ) = dbinom(2, size=3, prob=0.8) ## [1] 0.384 The expectation of this distribution can be shown to be \\[\\begin{aligned}E[X] &amp;= \\sum_{x=0}^{n}x\\,P(X=x) \\\\ &amp;= \\sum_{x=0}^{n}x\\;\\frac{n!}{x!\\left(n-x\\right)!}\\pi^{x}\\left(1-\\pi\\right)^{n-x}\\\\ &amp;= \\vdots \\\\ &amp;= n\\pi \\end{aligned}\\] and the variance can be similarly calculated \\[\\begin{aligned} V[X] &amp;= \\sum_{x=0}^{n}\\left(x-E\\left[X\\right]\\right)^{2}\\,P\\left(X=x|n,\\pi\\right) \\\\ &amp;= \\sum_{x=0}^{n}\\left(x-E\\left[X\\right]\\right)^{2}\\;\\frac{n!}{x!\\left(n-x\\right)!}\\pi^{x}\\left(1-\\pi\\right)^{n-x} \\\\ &amp;= \\vdots \\\\ &amp;= n\\pi(1-\\pi) \\end{aligned}\\] Example: Suppose a bird survey only captures the presence or absence of a particular bird (say the mountain chickadee). Assuming the true presence proportion at national forest sites around Flagstaff is \\(\\pi=0.1\\), then for \\(n=20\\) randomly chosen sites, the number of sites in which the bird was observed would have the following PMF. dist &lt;- data.frame( x = 0:20 ) %&gt;% mutate(probability = dbinom(x, size=20, prob=0.1)) ggplot(dist, aes(x=x)) + geom_point(aes(y=probability)) + geom_linerange(aes(ymax=probability, ymin=0)) + ggtitle(&#39;Binomial distribution: n=20, p=0.1&#39;) + xlab(&#39;Number of Sites Occupied&#39;) + theme_bw() Often we are interested in questions such as \\(P(X\\le2)\\) which is the probability that we see 2 or fewer of the sites being occupied by mountain chickadee. These calculations can be tedious to calculate by hand but R will calculate these cumulative distribution function values for you using the “p-function”. This cumulative distribution function gives the sum of all values up to and including the number given. # P(X=0) + P(X=1) + P(X=2) sum &lt;- dbinom(0, size=20, prob=0.1) + dbinom(1, size=20, prob=0.1) + dbinom(2, size=20, prob=0.1) sum ## [1] 0.6769268 # P(X &lt;= 2) pbinom(2, size=20, prob=0.1) ## [1] 0.6769268 In general we will be interested in asking four different questions about a distribution. What is the height of the probability mass function (or probability density function). For discrete variable \\(Y\\) this is \\(P\\left(Y=y\\right)\\) for whatever value of \\(y\\) we want. In R, this will be the d-function. What is the probability of observing a value less than or equal to \\(y\\)? In other words, to calculate \\(P\\left(Y\\le y\\right)\\). In R, this will be the p-function. What is a particular quantile of a distribution? For example, what value separates the lower \\(25\\%\\) from the upper \\(75\\%\\)? In R, this will be the q-function. Generate a random sample of values from a specified distribution. In R, this will be the r-function. 2.4.2 Poisson Distribution A commonly used distribution for count data is the Poisson. Number of customers arriving over a 5 minute interval Number of birds observed during a 10 minute listening period Number of prairie dog towns per 1000 hectares Number of alga clumps per cubic meter of lake water A discrete RV is a Poisson RV if the following conditions apply: Two or more events do not occur at precisely the same time or in the same space The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a non overlapping period or region. The expected number of events during one period or region, \\(\\lambda\\), is the same in all periods or regions of the same size. Assuming that these conditions hold for some count variable \\(Y\\), the the probability mass function is given by \\[P(Y=y)=\\frac{\\lambda^{y}e^{-\\lambda}}{y!}\\] where \\(\\lambda\\) is the expected number of events over 1 unit of time or space and \\(e\\) is the constant \\(2.718281828\\dots\\). \\[E[Y] = \\lambda\\] \\[Var[Y] = \\lambda\\] Example: Suppose we are interested in the population size of small mammals in a region. Let \\(Y\\) be the number of small mammals caught in a large trap over a 12 hour period. Finally, suppose that \\(Y\\sim Poisson(\\lambda=2.3)\\). What is the probability of finding exactly 4 critters in our trap? \\[P(Y=4) = \\frac{2.3^{4}\\,e^{-2.3}}{4!} = 0.1169\\] What about the probability of finding at most 4? \\[\\begin{aligned} P(Y\\le4) &amp;= P(Y=0)+P(Y=1)+P(Y=2)+P(Y=3)+P(Y=4) \\\\ &amp;= 0.1003+0.2306+0.2652+0.2033+0.1169 \\\\ &amp;= 0.9163 \\end{aligned}\\] What about the probability of finding 5 or more? \\[P(Y\\ge5) = 1-P(Y\\le4) = 1-0.9163 = 0.0837\\] These calculations can be done using the distribution function (d-function) for the Poisson and the cumulative distribution function (p-function). dist &lt;- data.frame( NumCaught = 0:10 ) %&gt;% mutate( probability = dpois( NumCaught, lambda=2.3 ) ) ggplot(dist, aes(x=NumCaught)) + geom_point( aes(y=probability) ) + geom_linerange(aes( ymax=probability, ymin=0)) + ggtitle(expression(paste(&#39;Poisson Distribution with &#39;, lambda == 2.3))) + labs(x=&#39;Number Caught&#39;) + theme_bw() # P( Y = 4) dpois(4, lambda=2.3) ## [1] 0.1169022 # P( Y &lt;= 4) ppois(4, lambda=2.3) ## [1] 0.9162493 # 1-P(Y &lt;= 4) == P( Y &gt; 4) == P( Y &gt;= 5) 1-ppois(4, 2.3) ## [1] 0.08375072 2.5 Continuous Random Variables Continuous random variables can take on an (uncountably) infinite number of values, and this results in a few obnoxious mathematical differences between how we handle continuous and discrete random variables. In particular, the probability that a continuous random variable \\(X\\) will take on a particular value will be zero, so we will be interested in finding the probability that the random variable is in some interval instead. Wherever we had a summation, \\(\\sum\\), we will instead have an integral, but because many students haven’t had calculus, we will resort to using R or tables of calculated values. 2.5.1 Uniform(0,1) Distribution Suppose you wish to draw a random number number between 0 and 1 and any two intervals of equal size should have the same probability of the value being in them. This random variable is said to have a Uniform(0,1) distribution. Because there are an infinite number of rational numbers between 0 and 1, the probability of any particular number being selected is \\(1/\\infty=0\\). But even though each number has 0 probability of being selected, some number must end up being selected. Because of this conundrum, probability theory doesn’t look at the probability of a single number, but rather focuses on a region of numbers. To make this distinction, we will define the distribution using a probability density function (pdf) instead of the probability mass function. In the discrete case, we had to constrain the probability mass function to sum to 1. In the continuous case, we have to constrain the probability density function to integrate to 1. Finding the area under the curve of a particular density function \\(f(x)\\) usually requires the use of calculus, but since this isn’t a calculus course, we will resort to using R or tables of calculated values. 2.5.2 Exponential Distribution The exponential distribution is the continuous analog of the Poisson distribution and is often used to model the time between occurrence of successive events. Perhaps we are modeling time between transmissions on a network, or the time between feeding events or prey capture. If the random variable \\(X\\) has an Exponential distribution, its probability density function is \\[f(x)=\\begin{cases} \\lambda e^{-\\lambda x} &amp; x\\ge0\\;\\textrm{ and }\\;\\lambda&gt;0\\\\ 0 &amp; \\textrm{otherwise} \\end{cases}\\] Analogous to the discrete distributions, we can define the Expectation and Variance of these distributions by replacing the summation with an integral \\[\\mu = E[X] = \\int_{0}^{\\infty}x\\,f(x)\\,dx = \\dots = \\frac{1}{\\lambda} \\] \\[\\sigma^2 = Var[X] = \\int_{0}^{\\infty}\\left(x-\\mu\\right)^{2}\\,f\\left(x\\right)\\,dx = \\dots = \\frac{1}{\\lambda^{2}}\\] Because the exponential distribution is defined by the rate of occurrence of an event, increasing that rate decreases the time between events. Furthermore because the rate of occurrence cannot be negative, we restrict \\(\\lambda&gt;0\\). Example: Suppose the time between insect captures \\(X\\) during a summer evening for a species of bat follows a exponential distribution with capture rate of \\(\\lambda=2\\) insects per minute and therefore the expected waiting time between captures is \\(1/\\lambda=1/2\\) minute. Suppose that we are interested in the probability that it takes a bat more than 1 minute to capture its next insect. \\[P(X&gt;1)=\\] data &lt;- data.frame(x=seq(0,5,length=1000), lambda = 2) %&gt;% mutate(y=dexp(x, rate = lambda), grp = ifelse( x &gt; 1, &#39;&gt; 1&#39;, &#39;&lt;= 1&#39;)) ggplot(data, aes(x=x, y=y, fill=grp)) + geom_area() + labs(y=&#39;density&#39;) + theme_bw() We now must resort to calculus to find this area. Or use tables of pre-calculated values. Or use R, remembering that p-functions give the area under the curve to the left of the given value. # P(X &gt; 1) == 1 - P(X &lt;= 1) ### Complement Rule 1 - pexp(1, rate=2) ## [1] 0.1353353 2.5.3 Normal Distribution Undoubtedly the most important distribution in statistics is the normal distribution. If my RV \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), its probability density function is given by \\[f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[\\frac{-(x-\\mu)^{2}}{2\\sigma^{2}}\\right]\\] where \\(\\exp[y]\\) is the exponential function \\(e^{y}\\). We could slightly rearrange the function to \\[f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right]\\] and see this distribution is defined by its expectation \\(E[X]=\\mu\\) and its variance \\(Var[X]=\\sigma^{2}\\). Notice I could define it using the standard deviation \\(\\sigma\\), and different software packages will expect it to be defined by one or the other. R defines the normal distribution using the standard deviation. Example: It is known that the heights of adult males in the US is approximately normal with a mean of 5 feet 10 inches (\\(\\mu=70\\) inches) and a standard deviation of \\(\\sigma=3\\) inches. Your instructor is a mere 5 feet 4 inches (64 inches). What proportion of the population is shorter than your professor? distr &lt;- data.frame(x=seq(57, 82, length=1000)) %&gt;% mutate( density = dnorm(x, mean=70, sd=3), group = ifelse(x&lt;=64, &#39;Shorter&#39;,&#39;Taller&#39;) ) ggplot(distr, aes(x=x, y=density, fill=group)) + geom_line() + geom_area() + theme_bw() Using R you can easily find this pnorm(64, mean=70, sd=3) ## [1] 0.02275013 2.5.4 Standardizing Before we had computers that could calculate these probabilities for any normal distribution, it was important to know how to convert a probability statement from an arbitrary \\(N\\left(\\mu,\\sigma^{2}\\right)\\) distribution to a question about a Standard Normal distribution, which is a normal distribution with mean \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). If we have \\[X\\sim N\\left(\\mu,\\sigma^{2}\\right)\\] then \\[Z=\\frac{X-\\mu}{\\sigma}\\sim N\\left(0,1\\right)\\] You might remember doing something similar in an undergraduate statistics course in order to use a table to look up some probability. From the height example, we calculate \\[\\begin{aligned}z &amp;= \\frac{64-70}{3} \\\\ &amp;= \\frac{-6}{3} \\\\ &amp;= -2 \\end{aligned}\\] Note that this calculation shows that he is \\(-2\\) standard deviations from the mean. Next we look at a table for \\(z=-2.00\\). To do this we go down to the \\(-2.0\\) row and over to the \\(.00\\) column and find \\(0.0228\\). Only slightly over 2% of the adult male population is shorter! How tall must a person be to be taller than \\(80\\%\\) of the rest of the adult male population? To answer that we must use the table in reverse and look for the \\(0.8\\) value. We find the closest value possible \\((0.7995)\\) and the \\(z\\) value associated with it is \\(z=0.84\\). Next we solve the standardizing equation for \\(x\\) \\[\\begin{aligned} z &amp;= \\frac{x-\\mu}{\\sigma} \\\\ 0.84 &amp;= \\frac{x-70}{3} \\\\ x &amp;= 3(0.84)+70 \\\\ &amp;= 72.49\\;\\textrm{inches} \\end{aligned}\\] Alternatively we could use the quantile function for the normal distribution (q-function) in R and avoid the imprecision of using a table. qnorm(.8, mean=0, sd=1) ## [1] 0.8416212 Empirical Rules: It is from the normal distribution that the empirical rules from the previous chapter is derived. If \\(X\\sim N(\\mu,\\sigma^{2})\\) then \\[\\begin{aligned} P(\\mu-\\sigma\\le X\\le\\mu+\\sigma) &amp;= P(-1 \\le Z \\le 1) \\\\ &amp;= P(Z \\le 1) - P(Z \\le -1) \\\\ &amp;\\approx 0.8413-0.1587 \\\\ &amp;= 0.6826 \\end{aligned}\\] 2.6 R Quick Reference We give a brief summary of the distributions used most in this couse and the abberviations used in R. Distribution Stem Parameters Parameter Interpretation Binomial binom size prob Number of Trials, Probability of Success (per Trial) Exponential exp rate Mean of the distribution Normal norm mean=0 sd=1 Center of the distribution, Standard deviation Uniform unif min=0 max=1 Minimum and Maximum of the distribution All the probability distributions available in R are accessed in exactly the same way, using a d-function, p-function, q-function, and r-function. Function Result d-function(x) The height of the probability distribution/density at \\(x\\) p-function(x) \\(P\\left(X\\le x\\right)\\) q-function(q) \\(x\\) such that \\(P\\left(X\\le x\\right) = q\\) r-function(n) \\(n\\) random observations from the distribution The mosaic package has versions of the p and q -functions that also print a out nice picture of the probabilities that you ask for. These functions are named by just adding an ‘x’ at the beginning of the function. For example mosaic::xpnorm(-1). 2.7 Exercises The population distribution of blood donors in the United States based on race/ethnicity and blood type as reported by the American Red Cross is given here: +————+——–+———+——–+——–+——–+ | \\(\\,\\) | O | A | B | AB | Total | +============+========+=========+========+========+========+ | White | 36% | 32.2% | 8.8% | 3.2% | \\(\\,\\) | +————+——–+———+——–+——–+——–+ | Black | 7% | 2.9% | 2.5% | 0.5% | \\(\\,\\) | +————+——–+———+——–+——–+——–+ | Asian | 1.7% | 1.2% | 1% | 0.3% | \\(\\,\\) | +————+——–+———+——–+——–+——–+ | Other | 1.5% | 0.8% | 0.3% | 0.1% | \\(\\,\\) | +————+——–+———+——–+——–+——–+ | \\(\\,\\) | \\(\\,\\) | \\(\\,\\) | \\(\\,\\) | \\(\\,\\) | 100% | +————+——–+———+——–+——–+——–+ Notice that the numbers given in the table sum to 100%, so the data presented are the probability of a particular ethnicity and blood type. Fill in the column and row totals. What is the probability that a randomly selected donor will be Asian and have Type O blood? That is to say, given a donor is randomly selected from the list of all donors, what is the probability that the selected donor will Asian with Type O? What is the probability that a randomly selected donor is white? That is to say, given a donor is randomly selected from the list of all donors, what is the probability that the selected donor is white? What is the probability that a randomly selected donor has Type A blood? That is to say, given a donor is selected from the list of all donors, what is the probability that the selected donor has Type A blood? What is the probability that a white donor will have Type A blood? That is to say, given a donor is randomly selected from the list of all the white donors, what is the probability that the selected donor has Type A blood? (Notice we already know the donor is white because we restricted ourselves to that subset!) Is blood type and ethnicity independent? Justify your response mathematically using your responses from the previous answers. For each of the following, mark if it is Continuous or Discrete. \\(\\underline{\\hspace{1in}}\\) Milliliters of tea drunk per day. \\(\\underline{\\hspace{1in}}\\) Different brands of soda drunk over the course of a year. \\(\\underline{\\hspace{1in}}\\) Number of days per week that you are on-campus for any amount of time. \\(\\underline{\\hspace{1in}}\\) Number of grizzly bears individuals genetically identified from a grid of hair traps in Glacier National Park. For each scenario, state whether the event should be modeled via a binomial or Poisson distribution. \\(\\underline{\\hspace{1in}}\\) Number of M&amp;Ms I eat per hour while grading homework \\(\\underline{\\hspace{1in}}\\) The number of mornings in the coming 7 days that I change my son’s first diaper of the day. \\(\\underline{\\hspace{1in}}\\) The number of Manzanita bushes per 100 meters of trail. During a road bike race, there is always a chance a crash will occur. Suppose the probability that at least one crash will occur in any race I’m in is \\(\\pi=0.2\\) and that races are independent. What is the probability that the next two races I’m in will both have crashes? What is the probability that neither of my next two races will have a crash? What is the probability that at least one of the next two races have a crash? My cats suffer from gastric distress due to eating house plants and the number of vomits per week that I have to clean up follows a Poisson distribution with rate \\(\\lambda=1.2\\) pukes per week. What is the probability that I don’t have to clean up any vomits this coming week? What is the probability that I must clean up 1 or more vomits? If I wanted to measure this process with a rate per day, what rate should I use? Suppose that the number of runners I see on a morning walk on the trails near my house has the following distribution (Notice I’ve never seen four or more runners on a morning walk): y 0 1 2 3 4+ Probabilty 0.45 0.25 0.20 0.0 What is the probability that I see 3 runners on a morning walk? What is the expected number of runners that I will encounter? What is the variance of the number of runners that I will encounter? If \\(Z\\sim N\\left(\\mu=0,\\sigma^{2}=1\\right)\\), find the following probabilities: \\(P(Z&lt;1.58)=\\) \\(P(Z=1.58)=\\) \\(P(Z&gt;-.27)=\\) \\(P(-1.97&lt;Z&lt;2.46)=\\) Using the Standard Normal Table or the table functions in R, find \\(z\\) that makes the following statements true. \\(P(Z&lt;z)=.75\\) \\(P(Z&gt;z)=.4\\) The amount of dry kibble that I feed my cats each morning can be well approximated by a normal distribution with mean \\(\\mu=200\\) grams and standard deviation \\(\\sigma=30\\) grams. What is the probability that I fed my cats more than 250 grams of kibble this morning? From my cats’ perspective, more food is better. How much would I have to feed them for this morning to be among the top \\(10\\%\\) of feedings? Sea lion weight is well approximated by a normal distribution with a mean of 300 kg and standard deviation of 15 kg. Use R to find the probability of randomly sampling a sea lion with a weight greater than 320 kg. Round your answer to 3 decimals. Now suppose we sample 10 sea lions. We wish to calculate the probability of how many sea lions will have a weight larger than 320 kg. What type of distribution will we have to use and what are the parameters of the distribution? Calculate by hand the probability of observing only 1 sea lion with a weight greater than 320 kg. Use R to calculate the probability of all possible outcomes. Graph the PMF of this distribution. "],
["3-confidence-intervals-via-bootstrapping.html", "Chapter 3 Confidence Intervals via Bootstrapping 3.1 Theory of Bootstrapping 3.2 Conducting a Bootstrap 3.3 Quantile-based Confidence Intervals 3.4 Additional Examples 3.5 Exercises", " Chapter 3 Confidence Intervals via Bootstrapping library(ggplot2) # graphing functions library(dplyr) # data summary tools library(boot) # bootstrap # Set default behavior of ggplot2 graphs to be black/white theme theme_set(theme_bw()) 3.1 Theory of Bootstrapping Suppose that we had a population of interest and we wish to estimate the mean of that population (the population mean we’ll denote as \\(\\mu\\)). We can’t observe every member of the population (which would be prohibitively expensive) so instead we take a random sample and from that sample calculate a sample mean (which we’ll denote \\(\\bar{x}\\)). We believe that \\(\\bar{x}\\) will be a good estimator of \\(\\mu\\), but it will vary from sample to sample and won’t be exactly equal to \\(\\mu\\). Next suppose we wish to ask if a particular value for \\(\\mu\\), say \\(\\mu_{0}\\), is consistent with our observed data? We know that \\(\\bar{x}\\) will vary from sample to sample, but we have no idea how much it will vary between samples. However, if we could understand how much \\(\\bar{x}\\) varied sample to sample, we could answer the question. For example, suppose that \\(\\bar{x}=5\\) and we know that \\(\\bar{x}\\) varied about \\(\\pm2\\) from sample to sample. Then I’d say that possible values of \\(\\mu_{0}\\) in the interval \\(3\\) to \\(7\\) \\(\\left(5\\pm2\\right)\\) are reasonable values for \\(\\mu\\) and anything outside that interval is not reasonable. Therefore, if we could take many, many repeated samples from the population and calculate our test statistic \\(\\bar{x}\\) for each sample, we could rule out possible values of \\(\\mu\\). Unfortunately we don’t have the time or money to repeatedly sample from the actual population, but we could sample from our best approximation to what the population is like. Suppose we were to sample from a population of shapes, and we observed \\(4/9\\) of the sample were squares, \\(3/9\\) were circles, and a triangle and a diamond. Then our best guess of what the population that we sampled from was a population with \\(4/9\\) squares, \\(3/9\\) circles, and \\(1/9\\) of triangles and diamonds. Using this approximated population (which is just many many copies of our sample data), we can repeatedly sample \\(\\bar{x}^{*}\\) values to create an estimate of the sampling distribution of \\(\\bar{x}\\). Because our approximate population is just an infinite number of copies of our sample data, then sampling from the approximate population is equivalent to sampling with replacement from our sample data. If I take \\(n\\) samples from \\(n\\) distinct objects with replacement, then the process can be thought of as mixing the \\(n\\) objects in a bowl and taking an object at random, noting which it is, replace it into the bowl, and then draw the next sample. Practically, this means some objects will be selected more than once and some will not be chosen at all. To sample our observed data with replacement, we can use the sample() function in R. names=c(&#39;Alison&#39;,&#39;Brandon&#39;,&#39;Casey&#39;,&#39;Derek&#39;,&#39;Elise&#39;) sample(names, length(names), replace=T) ## [1] &quot;Alison&quot; &quot;Derek&quot; &quot;Casey&quot; &quot;Alison&quot; &quot;Elise&quot; Notice Alison has selected twice, while Brandon has not been selected at all. The sampling from the estimated population via sampling from the observed data is called bootstrapping because we are making no distributional assumptions about where the data came from, and the idiom “Pulling yourself up by your bootstraps” seemed appropriate. Example: Mercury Levels in Fish from Florida Lakes A data set provided by the Lock\\(^{5}\\) introductory statistics textbook looks at the mercury levels in fish harvested from lakes in Florida. There are approximately 7,700 lakes in Florida that are larger than 10 acres. As part of a study to assess the average mercury contamination in these lakes, a random sample of \\(n=53\\) lakes, an unspecified number of fish were harvested and the average mercury level (in ppm) was calculated for fish in each lake. The goal of the study was to assess if the average mercury concentration was greater than the 1969 EPA “legally actionable level” of 0.5 ppm. # read the Lakes data set Lakes &lt;- read.csv(&#39;http://www.lock5stat.com/datasets/FloridaLakes.csv&#39;) # make a nice picture... dot plots are very similar to histograms # dot plots can be informative for small samples ggplot(Lakes, aes(x=AvgMercury)) + geom_dotplot() We can calculate mean average mercury level for the \\(n=53\\) lakes Lakes %&gt;% summarise(xbar = mean( AvgMercury )) ## xbar ## 1 0.5271698 The sample mean is greater than \\(0.5\\) but not by too much. Is a true population mean concentration \\(\\mu_{Hg}\\) that is \\(0.5\\) or less incompatible with our observed data? Is our data sufficient evidence to conclude that the average mercury content is greater than \\(0.5\\)? Perhaps the true average mercury content is less than (or equal to) \\(0.5\\) and we just happened to get a random sample that with a mean greater than \\(0.5\\)? 3.2 Conducting a Bootstrap The first step in answering these questions is to create an estimate of the sampling distribution of \\(\\bar{x}_{Hg}\\). To do this, we will sample from the approximate population of lakes, which is just many many replicated copies of our sample data. There are many ways to bootstrap using R, and chosen here is to introduce the package boot for conducting the bootstrap for us with minimal code. For alternative methods using base R or the package mosaic, see Appendix A. library(boot) To use the boot() function within the boot package, we will have to define a function for the resampling to occur. Below, we create the function mean.function, that accepts a vector (our observations) and calculates the mean. The index is so that boot() can do the resampling. How do you think we could change this to bootstrap different statistics? mean.function &lt;- function(x, index) { d &lt;- x[index] return(mean(d)) } Once you have defined what you would like to bootstrap, the function boot() is a simple call in R, and produces the number of iterations \\(R\\) we choose. Let us try running \\(R = 10000\\) bootstrap iterations. # create the Estimated Sampling Distribution of xbar BootDist &lt;- boot(data = Lakes$AvgMercury, statistic = mean.function, R=10000) There are many outputs available within the output of boot(). We are interested in the calculated statistic for each redraw, which is saved within the output as the variable \\(t\\). We can place the calculated means for each redraw into a data frame and produce a visualization of the estimated sampling distribution of \\(\\bar{x}\\). # The first few calculated means. head(BootDist$t) ## [,1] ## [1,] 0.5141509 ## [2,] 0.5150943 ## [3,] 0.6043396 ## [4,] 0.4762264 ## [5,] 0.4950943 ## [6,] 0.4137736 # show a histogram of the estimated sampling distribution of xbar BootDist.graph &lt;- data.frame(xbar=BootDist$t) ggplot(BootDist.graph, aes(x=xbar)) + geom_histogram() + ggtitle(&#39;Estimated Sampling distribution of xbar&#39; ) 3.3 Quantile-based Confidence Intervals In many cases we have seen, the sampling distribution of a statistic is centered on the parameter we are interested in estimating and is symmetric about that parameter. There are actually several ways to create a confidence interval from the estimated sampling distribution. The method presented here is called the “percentile” method and works when the sampling distribution is symmetric and the estimator we are using is unbiased. For example, we expect that the sample mean \\(\\bar{x}\\) should be a good estimate of the population mean \\(\\mu\\) and the sampling distribution of \\(\\bar{x}\\) should look something like the following. There are two points, (call them \\(L\\) and \\(U\\)) where for our given sample size and population we are sampling from, where we expect that \\(95\\%\\) of the sample means to fall within. That is to say, \\(L\\) and \\(U\\) capture the middle \\(95\\%\\) of the sampling distribution of \\(\\bar{x}\\). These sample means are randomly distributed about the population mean \\(\\mu\\). Given our sample data and sample mean \\(\\bar{x}\\), we can examine how our simulated values of \\(\\bar{x}^{*}\\) vary about \\(\\bar{x}\\). I expect that these simulated sample means \\(\\bar{x}^{*}\\) should vary about \\(\\bar{x}\\) in the same way that \\(\\bar{x}\\) values vary around \\(\\mu\\). Below are three estimated sampling distributions that we might obtain from three different samples and their associated sample means. For each possible sample, we could consider creating the estimated sampling distribution of \\(\\bar{X}\\) and calculating the \\(L\\) and \\(U\\) values that capture the middle \\(95\\%\\) of the estimated sampling distribution. Below are twenty samples, where we’ve calculated this interval for each sample. Most of these intervals contain the true parameter \\(\\mu\\), that we are trying to estimate. In practice, I will only take one sample and therefore will only calculate one sample mean and one interval, but I want to recognize that the method I used to produce the interval (i.e. take a random sample, calculate the mean and then the interval) will result in intervals where only \\(95\\%\\) of those intervals will contain the mean \\(\\mu\\). Therefore, I will refer to the interval as a \\(95\\%\\) confidence interval. After the sample is taken and the interval is calculated, the numbers lower and upper bounds of the confidence interval are fixed. Because \\(\\mu\\) is a constant value and the confidence interval is fixed, nothing is changing. To distinguish between a future random event and the fixed (but unknown) outcome of if I ended up with an interval that contains \\(\\mu\\) and we use the term confidence interval instead of probability interval. # calculate the 95% confidence interval using middle 95% of xbars quantile( BootDist$t, probs=c(.025, .975) ) ## 2.5% 97.5% ## 0.4358443 0.6196226 There are several ways to interpret this interval. The process used to calculate this interval (take a random sample, calculate a statistic, repeatedly re-sample, and take the middle \\(95\\%\\)) is a process that results in an interval that contains the parameter of interest on \\(95\\%\\) of the samples we could have collected, however we don’t know if the particular sample we collected and its resulting interval of \\(\\left(0.44,\\,0.62\\right)\\) is one of the intervals containing \\(\\mu\\). We are \\(95\\%\\) confident that \\(\\mu\\) is in the interval \\(\\left(0.44,\\,0.62\\right)\\). This is delightfully vague and should be interpreted as a shorter version of the previous interpretation. The interval \\(\\left(0.44,\\,0.62\\right)\\) is the set of values of \\(\\mu\\) that are consistent with the observed data at the \\(0.05\\) threshold of statistical significance for a two-sided hypothesis test 3.4 Additional Examples Example: Fuel Economy Suppose we have data regarding fuel economy of \\(5\\) new vehicles of the same make and model and we wish to test if the observed fuel economy is consistent with the advertised \\(31\\) mpg at highway speeds. Here are the data: CarMPG &lt;- data.frame( ID=1:5, mpg = c(31.8, 32.1, 32.5, 30.9, 31.3) ) CarMPG %&gt;% summarise( xbar=mean(mpg) ) ## xbar ## 1 31.72 We will use the sample mean to assess if the sample fuel efficiency is consistent with the advertised number. Because these cars could be considered a random sample of all new cars of this make, we will create the estimated sampling distribution using the bootstrap re-sampling of the data. # Run the bootstrap now with CarMPG$mpg as our data BootDist &lt;- boot(data = CarMPG$mpg, statistic = mean.function, R=10000) # show a histogram of the sampling distribution of xbar BootDist.graph &lt;- data.frame(xbar=BootDist$t) ggplot(BootDist.graph, aes(x=xbar)) + geom_histogram() + ggtitle(&#39;Sampling Distribution of mean(mpg)&#39;) # calculate the 95% confidence interval using middle 95% of xbars quantile( BootDist$t, probs=c(.025, .975) ) ## 2.5% 97.5% ## 31.22 32.20 We see that the \\(95\\%\\) confidence interval is \\(\\left(31.2,\\,32.2\\right)\\) and does not actually contain the advertised \\(31\\) mpg. However, I don’t think we would object to a car manufacturer selling us a car that is better than advertised. Example: Pulse Rate of College Students In the package Lock5Data, the dataset GPAGender contains information taken from undergraduate students in an Introductory Statistics course. This is a convenience sample, but could be considered representative of students at that university. One of the covariates measured was the students pulse rate and we will use this to create a confidence interval for average pulse of students at that university. First we’ll look at the raw data. data(GPAGender, package=&#39;Lock5Data&#39;) # load the dataset # Now a nice histogram ggplot(GPAGender, aes(x=Pulse, y=..density..)) + geom_histogram(binwidth=2) + ggtitle(&#39;Sample Data&#39;) It is worth noting this was supposed to be measuring resting heart rates, but there are two students had extremely high pulse rates and six with extremely low rates. The two high values are approximately what you’d expect from someone currently engaged in moderate exercise and the low values are levels we’d expect from highly trained endurance athletes. # Summary Statistics GPAGender %&gt;% summarise(xbar = mean(Pulse), StdDev = sd(Pulse)) ## xbar StdDev ## 1 69.90379 12.08569 So the sample mean is \\(\\bar{x}=69.9\\) but how much should we expect our sample mean to vary from sample to sample when our sample size is \\(n=343\\) people? We’ll estimate the sampling distribution of \\(\\bar{X}\\) using the bootstrap. # Create the bootstrap replicates BootDist &lt;- boot(data = GPAGender$Pulse, statistic = mean.function, R=10000) # show a histogram of the sampling distribution of xbar BootDist.graph &lt;- data.frame(xbar=BootDist$t) ggplot(BootDist.graph, aes(x=xbar)) + geom_histogram() + ggtitle(&#39;Sampling Distribution of mean(Pulse)&#39;) quantile( BootDist$t, probs=c(.025, .975) ) ## 2.5% 97.5% ## 68.65007 71.17515 Based on the quantile approach, the \\(95\\%\\) bootstrap confidence for the mean pulse rate of undergraduates in the introductory statistics course is \\(68.7\\) to \\(71.2\\) beats per minutes. 3.5 Exercises For several of these exercises, we will use data sets from the R package Lock5Data, which greatly contributed to the pedagogical approach of these notes. Install the package from CRAN using the RStudio point-and-click interface Tools -&gt; Install Packages…. Load the dataset BodyTemp50 from the Lock5Data package. This is a dataset of 50 healthy adults. One of the columns of this dataset is the Pulse of the 50 data points, which is the number of heartbeats per minute. Create a histogram of the observed pulse values. Comment on the graph and aspects of the graph that might be of scientific interest. Below will help you load the data, and we want to use the Pulse variable. data( BodyTemp50, package=&#39;Lock5Data&#39; ) #?BodyTemp50 Calculate the sample mean \\(\\bar{x}\\) and sample standard deviation \\(s\\) of the pulses. Create a dataset of 10000 bootstrap replicates of \\(\\bar{x}^{*}\\). Create a histogram of the bootstrap replicates. Calculate the mean and standard deviation of this distribution. Notice that the standard deviation of the distribution is often called the Standard Error of \\(\\bar{x}\\) and we’ll denote it as \\(\\hat{\\sigma}_{\\bar{x}}\\). Using the bootstrap replicates, create a 95% confidence interval for \\(\\mu\\), the average adult heart rate. Calculate the interval \\[\\left(\\bar{x}-2\\cdot\\hat{\\sigma}_{\\bar{x}}\\,,\\,\\;\\;\\bar{x}+2\\cdot\\hat{\\sigma}_{\\bar{x}}\\right)\\] and comment on its similarity to the interval you calculated in part (e). Load the dataset EmployedACS from the Lock5Data package. This is a dataset drawn from American Community Survey results which is conducted monthly by the US Census Bureau and should be representative of US workers. The column HoursWk represents the number of hours worked per week. Create a histogram of the observed hours worked. Comment on the graph and aspects of the graph that might be of scientific interest. Calculate the sample mean \\(\\bar{x}\\) and sample standard deviation \\(s\\) of the worked hours per week. Create a dataset of 10000 bootstrap replicates of \\(\\bar{x}^{*}\\). Create a histogram of the bootstrap replicates. Calculate the mean and standard deviation of this distribution. Notice that the standard deviation of the distribution is often called the Standard Error of \\(\\bar{x}\\) and we’ll denote it as \\(\\sigma_{\\bar{x}}\\). Using the bootstrap replicates, create a 95% confidence interval for \\(\\mu\\), the average worked hours per week. Calculate the interval \\[\\left(\\bar{x}-2\\cdot\\hat{\\sigma}_{\\bar{x}}\\,,\\,\\;\\;\\bar{x}+2\\cdot\\hat{\\sigma}_{\\bar{x}}\\right)\\] and comment on its similarity to the interval you calculated in part (e). "],
["appendix-a.html", "Appendix A", " Appendix A This method uses the mosaic package and can work very well when everything is in data frames. # create the Estimated Sampling Distribution of xbar BootDist &lt;- mosaic::do(10000) * mosaic::resample(Lakes) %&gt;% summarise(xbar = mean(AvgMercury)) # what columns does the data frame &quot;BootDist&quot; have? head(BootDist) ## xbar ## 1 0.5986792 ## 2 0.4700000 ## 3 0.5337736 ## 4 0.5311321 ## 5 0.5245283 ## 6 0.6045283 # show a histogram of the estimated sampling distribution of xbar ggplot(BootDist, aes(x=xbar)) + geom_histogram() + ggtitle(&#39;Estimated Sampling distribution of xbar&#39; ) "]
]
